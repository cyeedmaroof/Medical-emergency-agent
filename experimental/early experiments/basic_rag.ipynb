{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this file i am implementing simple RAG architicture. ###\n",
    "\n",
    "#### Fetch the doc ####\n",
    "#### Split the docs, embed and store in vetor store ####\n",
    "#### fectch the relivent docs and add them as context to the prompt ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import get_azure_openai_model, get_azure_openai_mini_model\n",
    "\n",
    "model = get_azure_openai_model()\n",
    "model2 = get_azure_openai_mini_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pdf_file \u001b[38;5;129;01min\u001b[39;00m pdf_files:\n\u001b[32m     10\u001b[39m     loader = PyPDFLoader(os.path.join(folder_path, pdf_file))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m loader.alazy_load():\n\u001b[32m     12\u001b[39m         pages.append(page)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:83\u001b[39m, in \u001b[36mBaseLoader.alazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     81\u001b[39m done = \u001b[38;5;28mobject\u001b[39m()\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     doc = \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mnext\u001b[39m, iterator, done)  \u001b[38;5;66;03m# type: ignore[call-arg, arg-type]\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m done:\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:588\u001b[39m, in \u001b[36mrun_in_executor\u001b[39m\u001b[34m(executor_or_config, func, *args, **kwargs)\u001b[39m\n\u001b[32m    584\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    587\u001b[39m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(\n\u001b[32m    589\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    590\u001b[39m         cast(Callable[..., T], partial(copy_context().run, wrapper)),\n\u001b[32m    591\u001b[39m     )\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load PDFs\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "folder_path = \"papers\"\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "pages = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(os.path.join(folder_path, pdf_file))\n",
    "    async for page in loader.alazy_load():\n",
    "        pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'papers\\\\Alexander Zai, Brandon Brown - Deep Reinforcement Learning in Action-Manning Publications (2020).pdf', 'page': 72}, page_content='51Predicting future rewards: Value and policy functions\\nIn the mathematical notation, s is a state and Pr(A | s) is a probability distribution over\\nthe set of actions A, given state s. The probability of each action in the distribution is\\nthe probability that the action will produce the greatest reward.\\n2.7.2 Optimal policy \\nThe policy is the part of our reinforcement learning algorithm that chooses actions\\ngiven its current state. We can then formulate the optimal policy —it’s the strategy that\\nmaximizes rewards.\\nRemember, a particular policy is a map or function, so we have some sort of set of pos-\\nsible policies; the optimal policy is just an argmax (which selects the maximum) over\\nthis set of possible policies as a function of their expected rewards.\\n Again, the whole goal of a reinforcement learning algorithm (our agent) is to\\nchoose the actions that lead to the maximal expected rewards. But there are two ways\\nwe can train our agent to do this: \\n\\uf0a1 Directly—We can teach the agent to learn what actions are best, given what state\\nit is in.\\n\\uf0a1 Indirectly—We can teach the agent to learn which states are most valuable, and\\nthen to take actions that lead to the most valuable states. \\nThis indirect method leads us to the idea of value functions.\\n2.7.3 Value functions\\nValue functions are functions that map a state or a state-action pair to the expected value\\n(the expected reward) of being in some state or taking some action in some state. You\\nmay recall from statistics that the expected reward is just the long-term average of\\nrewards received after being in some state or taking some action. When we speak of\\nthe value function, we usually mean a state-value function. \\nTable 2.5 The policy function\\nMath English\\nπ; s → Pr(A⏐s), where s ∈ S A policy, π, is a mapping from states to the (probabilistically) best \\nactions for those states.\\nTable 2.6 The optimal policy\\nMath English\\nπ∗ = argmax E(R⏐π), If we know the expected rewards for following any possible policy, π, the opti-\\nmal policy, π∗, is a policy that, when followed, produces the maximum possi-\\nble rewards.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add docs to vector store\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 97: 76 CHAPTER 3 Predicting the best states and actions: Deep Q-networks\n",
      "3.3.2 Experience replay\n",
      "Catastrophic forgetting is probably not something we have to worry about with the first\n",
      "variant of our game because the targets are always stationary, and indeed the model\n",
      "successfully learned how to play it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vector search similarity test\n",
    "\n",
    "docs = vector_store.similarity_search(\"What is the replay memory?\", k=1)\n",
    "for doc in docs:\n",
    "    print(f'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrival\n",
    "retriever = vector_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 14: xiii\n",
      "preface\n",
      "Deep reinforcement learning was launched into the spotlight in 2015, when Deep-\n",
      "Mind produced an algorithm capable of playing a suite of Atari 2600 games at super-\n",
      "human performance. Artificial intelligence seemed to be finally making some real\n",
      "progress, and we wanted to be a part of it\n",
      "\n",
      "Page 14: xiii\n",
      "preface\n",
      "Deep reinforcement learning was launched into the spotlight in 2015, when Deep-\n",
      "Mind produced an algorithm capable of playing a suite of Atari 2600 games at super-\n",
      "human performance. Artificial intelligence seemed to be finally making some real\n",
      "progress, and we wanted to be a part of it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This prompt provides instructions to the model. \n",
    "# The prompt includes the query and the source, which are specified further down in the code.\n",
    "GROUNDED_PROMPT=\"\"\"\n",
    "You are a friendly assistant that helps students.\n",
    "Answer the query using only the sources provided below in a friendly and concise bulleted manner.\n",
    "Answer ONLY with the facts listed in the list of sources below.\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the sources below.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\"\n",
    "\n",
    "# The query is sent to the search engine, but it's also passed in the prompt\n",
    "query=\"what is deeplearning?\"\n",
    "\n",
    "# Retrieve the selected fields from the search index related to the question\n",
    "retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "for doc in retrieved_docs:\n",
    "    print(f'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The term \"deep learning\" refers to a significant advancement in artificial intelligence, particularly evident since the deep learning revolution around 2012.\n",
      "- Deep reinforcement learning gained attention in 2015 when DeepMind created an algorithm that outperformed humans in playing Atari 2600 games. \n",
      "- Deep learning combines machine learning techniques with neural network architectures, emphasizing its effectiveness in various applications.\n",
      "- Individuals with software engineering backgrounds can understand the fundamentals of deep reinforcement learning despite the advanced mathematics involved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "response = model2.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": GROUNDED_PROMPT.format(query=query, sources=retrieved_docs)\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini-test\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
