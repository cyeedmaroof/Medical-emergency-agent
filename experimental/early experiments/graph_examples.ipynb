{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d362591e-9f54-46f9-b703-49062ac3072f",
   "metadata": {},
   "source": [
    "# Knowledge Graph RAG\n",
    "\n",
    "<img src=\"./media/graph_start.png\" width=600>\n",
    "\n",
    "*[Improving Knowledge Graph Completion with Generative LM and neighbors](https://deeppavlov.ai/research/tpost/bn15u1y4v1-improving-knowledge-graph-completion-wit)*\n",
    "\n",
    "In the evolving landscape of AI and information retrieval, knowledge graphs have emerged as a powerful way to represent complex, interconnected information. A knowledge graph is a knowledge base that uses a graph-structured data model or topology to represent and operate on data. Knowledge graphs are often used to store interlinked descriptions of entities – objects, events, situations or abstract concepts – while also encoding the free-form semantics or relationships underlying these entities. [Source: Wikipedia](https://en.wikipedia.org/wiki/Knowledge_graph)\n",
    "\n",
    "What makes knowledge graphs particularly powerful is their ability to mirror human cognition in data. They more explicitly map the relationships between objects, concepts, or ideas together through both their semantic and relational connections. This approach closely parallels how our brains naturally understand and internalize information – not as isolated facts, but as a web of interconnected concepts and relationships.\n",
    "\n",
    "<img src=\"./media/coffee_graph_ex.png\" width=400>\n",
    "\n",
    "Looking at a concept like \"coffee,\" we don't just know it's a beverage; we automatically connect it to related concepts like beans, brewing methods, caffeine, morning routines, and social interactions. Knowledge graphs capture these natural associations in a structured way.\n",
    "\n",
    "Traditional RAG systems, while effective at semantic similarity-based retrieval, often struggle to capture broader conceptual relationships across text chunks. Knowledge Graph RAG addresses this limitation by introducing a structured, hierarchical approach to information organization and retrieval. By representing data in a graph format, these systems can traverse relationships between concepts, enabling more sophisticated query understanding and response generation. This approach allows for targeted querying along specific relationship paths, handles complex multi-hop questions, and provides clearer reasoning through explicit connection paths. The result is a more nuanced and interpretable system that combines the structured reasoning of knowledge graphs with the natural language capabilities of large language models.\n",
    "\n",
    "While [knowledge graphs are not a new concept](https://blog.google/products/search/introducing-knowledge-graph-things-not/), their creation has traditionally been a resource-intensive process. Early knowledge graphs were built either through manual curation by domain experts or by converting existing structured data from relational databases. This limited both their scale and adaptability to new domains.\n",
    "\n",
    "<img src=\"./media/table_comp.png\" width=600>\n",
    "\n",
    "*[What is a Knowledge Graph (KG)?](https://zilliz.com/learn/what-is-knowledge-graph)*\n",
    "\n",
    "The introduction of LLMs has transformed this landscape. LLMs' capabilities in NLP, reasoning, and relationship extraction now enable automated construction of knowledge graphs from unstructured text. These models can identify entities, infer relationships, and structure information in ways that previously required extensive manual labor. As a plus, this allows knowledge graphs to be dynamically updated and expanded as new information becomes available, making them more practical and scalable for real-world applications.\n",
    "\n",
    "To see this in action ourselves, and compare it to traditional vector similarity techniques, we'll take a look at Microsoft's Open Source [GraphRAG](https://microsoft.github.io/graphrag/) and how it works behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb752dad-d6bf-436f-a175-03a1d491bb3e",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 Main Components of Knowledge Graphs\n",
    "\n",
    "**Entity**\n",
    "\n",
    "<img src=\"./media/entities.png\" width=500>\n",
    "\n",
    "An Entity is a distinct object, person, place, event, or concept that has been extracted from a chunk of text through LLM analysis. Entities form the nodes of the knowledge graph. During the creation of the knowledge graph, when duplicate entities are found they are merged while preserving their various descriptions, creating a comprehensive representation of each unique entity.\n",
    "\n",
    "**Relationship**\n",
    "\n",
    "<img src=\"./media/relationship.png\" width=400>\n",
    "\n",
    "A Relationship defines a connection between two entities in the knowledge graph. These connections are extracted directly from text units through LLM analysis, alongside entities. Each relationship includes a source entity, target entity, and descriptive information about their connection. When duplicate relationships are found between the same entities, they are merged by combining their descriptions to create a more complete understanding of the connection.\n",
    "\n",
    "**Community**\n",
    "\n",
    "<img src=\"./media/communities.png\" width=400>\n",
    "\n",
    "A Community is a cluster of related entities and relationships identified through hierarchical community detection, generally using the [Leiden Algorithm](https://en.wikipedia.org/wiki/Leiden_algorithm). Communities create a structured way to understand different levels of granularity within the knowledge graph, from broad overviews at the top level to detailed local clusters at lower levels. This hierarchical structure helps in organizing and navigating complex knowledge graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7e77f-a543-49e2-813e-c1305f7a058d",
   "metadata": {},
   "source": [
    "---\n",
    "## GraphRAG Creation Data Flow\n",
    "\n",
    "<img src=./media/graph_building.png width=1000>\n",
    "\n",
    "Indexxing in GraphRAG is an extensive process, where we load the document, split it into chunks, create sub graphs at a chunk level, combine these subgraphs into our final graph, algorithmically identify communities, then document the communities main features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19896a38-bcf7-4664-9f57-cb12cf15cdea",
   "metadata": {},
   "source": [
    "### **Loading and Splitting Our Text**\n",
    "\n",
    "For our example, we'll be using [The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities](https://arxiv.org/pdf/2408.13296).\n",
    "\n",
    "This will be loaded as a text file (remove index, glossary, and references) and split into 1200 token, 100 token overlap chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6586b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./kgdata/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97ce9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Characters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.md</td>\n",
       "      <td>2656</td>\n",
       "      <td>8569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02.md</td>\n",
       "      <td>2139</td>\n",
       "      <td>7333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03.md</td>\n",
       "      <td>1944</td>\n",
       "      <td>6596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04.md</td>\n",
       "      <td>2050</td>\n",
       "      <td>6934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05.md</td>\n",
       "      <td>3078</td>\n",
       "      <td>10185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>06.md</td>\n",
       "      <td>3815</td>\n",
       "      <td>12437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07.md</td>\n",
       "      <td>3717</td>\n",
       "      <td>11991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08.md</td>\n",
       "      <td>2038</td>\n",
       "      <td>6993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09.md</td>\n",
       "      <td>2548</td>\n",
       "      <td>8475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.md</td>\n",
       "      <td>3056</td>\n",
       "      <td>9977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.md</td>\n",
       "      <td>5124</td>\n",
       "      <td>16805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.md</td>\n",
       "      <td>2098</td>\n",
       "      <td>6610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.md</td>\n",
       "      <td>2500</td>\n",
       "      <td>8434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.md</td>\n",
       "      <td>2884</td>\n",
       "      <td>9763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.md</td>\n",
       "      <td>1842</td>\n",
       "      <td>6012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19.md</td>\n",
       "      <td>3148</td>\n",
       "      <td>10322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20.md</td>\n",
       "      <td>3700</td>\n",
       "      <td>11903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21.md</td>\n",
       "      <td>2554</td>\n",
       "      <td>8371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>23.md</td>\n",
       "      <td>3520</td>\n",
       "      <td>11676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24.md</td>\n",
       "      <td>3784</td>\n",
       "      <td>12707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25.md</td>\n",
       "      <td>2947</td>\n",
       "      <td>10397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>26.md</td>\n",
       "      <td>3501</td>\n",
       "      <td>11353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>27.md</td>\n",
       "      <td>4364</td>\n",
       "      <td>14109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>28.md</td>\n",
       "      <td>2761</td>\n",
       "      <td>9131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29.md</td>\n",
       "      <td>2083</td>\n",
       "      <td>7066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30.md</td>\n",
       "      <td>1654</td>\n",
       "      <td>5594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>31.md</td>\n",
       "      <td>2942</td>\n",
       "      <td>9839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32.md</td>\n",
       "      <td>2927</td>\n",
       "      <td>9918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>33.md</td>\n",
       "      <td>4112</td>\n",
       "      <td>12702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>34.md</td>\n",
       "      <td>3072</td>\n",
       "      <td>10165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>35.md</td>\n",
       "      <td>3302</td>\n",
       "      <td>10657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>36.md</td>\n",
       "      <td>2495</td>\n",
       "      <td>8213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>37.md</td>\n",
       "      <td>2720</td>\n",
       "      <td>9028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>38.md</td>\n",
       "      <td>2346</td>\n",
       "      <td>7866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>39.md</td>\n",
       "      <td>2498</td>\n",
       "      <td>8391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>40.md</td>\n",
       "      <td>2237</td>\n",
       "      <td>7541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>41.md</td>\n",
       "      <td>1744</td>\n",
       "      <td>6064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document  Tokens  Characters\n",
       "0     01.md    2656        8569\n",
       "1     02.md    2139        7333\n",
       "2     03.md    1944        6596\n",
       "3     04.md    2050        6934\n",
       "4     05.md    3078       10185\n",
       "5     06.md    3815       12437\n",
       "6     07.md    3717       11991\n",
       "7     08.md    2038        6993\n",
       "8     09.md    2548        8475\n",
       "9     10.md    3056        9977\n",
       "10    11.md    5124       16805\n",
       "11    12.md    2098        6610\n",
       "12    13.md    2500        8434\n",
       "13    14.md    2884        9763\n",
       "14    15.md    1842        6012\n",
       "15    19.md    3148       10322\n",
       "16    20.md    3700       11903\n",
       "17    21.md    2554        8371\n",
       "18    23.md    3520       11676\n",
       "19    24.md    3784       12707\n",
       "20    25.md    2947       10397\n",
       "21    26.md    3501       11353\n",
       "22    27.md    4364       14109\n",
       "23    28.md    2761        9131\n",
       "24    29.md    2083        7066\n",
       "25    30.md    1654        5594\n",
       "26    31.md    2942        9839\n",
       "27    32.md    2927        9918\n",
       "28    33.md    4112       12702\n",
       "29    34.md    3072       10165\n",
       "30    35.md    3302       10657\n",
       "31    36.md    2495        8213\n",
       "32    37.md    2720        9028\n",
       "33    38.md    2346        7866\n",
       "34    39.md    2498        8391\n",
       "35    40.md    2237        7541\n",
       "36    41.md    1744        6064"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens across all documents: 105900\n",
      "Average tokens per document: 2862.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "# Initialize the cl100k_base tokenizer (used by GPT-4 models)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Function to count tokens in a text\n",
    "def count_tokens(text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Create a dataframe with document information and token counts\n",
    "doc_info = []\n",
    "total_tokens = 0\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    filename = doc.metadata.get('file_name', f'Document {i}')\n",
    "    token_count = count_tokens(doc.text)\n",
    "    total_tokens += token_count\n",
    "    doc_info.append({\n",
    "        'Document': filename,\n",
    "        'Tokens': token_count,\n",
    "        'Characters': len(doc.text)\n",
    "    })\n",
    "\n",
    "# Create DataFrame and display\n",
    "doc_df = pd.DataFrame(doc_info)\n",
    "display(doc_df)\n",
    "\n",
    "print(f\"Total tokens across all documents: {total_tokens}\")\n",
    "print(f\"Average tokens per document: {total_tokens / len(documents):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b09a75ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents into chunks (size=1200, overlap=100)\n",
      "Processing documents: 0/37 (0%)\n",
      "Processing documents: 1/37 (2.7%) - 136.5 items/s\n",
      "Processing documents: 2/37 (5.4%) - 168.4 items/s\n",
      "Processing documents: 3/37 (8.1%) - 197.8 items/s\n",
      "Processing documents: 4/37 (10.8%) - 215.0 items/s\n",
      "Processing documents: 5/37 (13.5%) - 211.2 items/s\n",
      "Processing documents: 6/37 (16.2%) - 223.9 items/s\n",
      "Processing documents: 7/37 (18.9%) - 225.0 items/s\n",
      "Processing documents: 8/37 (21.6%) - 257.1 items/s\n",
      "Processing documents: 9/37 (24.3%) - 249.0 items/s\n",
      "Processing documents: 10/37 (27.0%) - 276.7 items/s\n",
      "Processing documents: 11/37 (29.7%) - 261.1 items/s\n",
      "Processing documents: 12/37 (32.4%) - 284.8 items/s\n",
      "Processing documents: 13/37 (35.1%) - 308.6 items/s\n",
      "Processing documents: 14/37 (37.8%) - 332.3 items/s\n",
      "Processing documents: 15/37 (40.5%) - 356.1 items/s\n",
      "Processing documents: 16/37 (43.2%) - 379.8 items/s\n",
      "Processing documents: 17/37 (45.9%) - 273.4 items/s\n",
      "Processing documents: 18/37 (48.6%) - 280.0 items/s\n",
      "Processing documents: 19/37 (51.4%) - 286.2 items/s\n",
      "Processing documents: 20/37 (54.1%) - 301.3 items/s\n",
      "Processing documents: 21/37 (56.8%) - 316.3 items/s\n",
      "Processing documents: 22/37 (59.5%) - 290.2 items/s\n",
      "Processing documents: 23/37 (62.2%) - 289.0 items/s\n",
      "Processing documents: 24/37 (64.9%) - 301.6 items/s\n",
      "Processing documents: 25/37 (67.6%) - 314.1 items/s\n",
      "Processing documents: 26/37 (70.3%) - 326.7 items/s\n",
      "Processing documents: 27/37 (73.0%) - 339.3 items/s\n",
      "Processing documents: 28/37 (75.7%) - 303.8 items/s\n",
      "Processing documents: 29/37 (78.4%) - 299.3 items/s\n",
      "Processing documents: 30/37 (81.1%) - 309.7 items/s\n",
      "Processing documents: 31/37 (83.8%) - 320.0 items/s\n",
      "Processing documents: 32/37 (86.5%) - 330.3 items/s\n",
      "Processing documents: 33/37 (89.2%) - 340.6 items/s\n",
      "Processing documents: 34/37 (91.9%) - 311.9 items/s\n",
      "Processing documents: 35/37 (94.6%) - 312.6 items/s\n",
      "Processing documents: 36/37 (97.3%) - 315.8 items/s\n",
      "Processing documents: 37/37 (100.0%) - 321.9 items/s\n",
      "Created 126 total chunks across 37 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Chunks</th>\n",
       "      <th>Avg. Chunk Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.md</td>\n",
       "      <td>6</td>\n",
       "      <td>3013.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>27.md</td>\n",
       "      <td>5</td>\n",
       "      <td>3036.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>33.md</td>\n",
       "      <td>5</td>\n",
       "      <td>2749.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>35.md</td>\n",
       "      <td>4</td>\n",
       "      <td>2854.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>26.md</td>\n",
       "      <td>4</td>\n",
       "      <td>3045.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24.md</td>\n",
       "      <td>4</td>\n",
       "      <td>3408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>31.md</td>\n",
       "      <td>4</td>\n",
       "      <td>2682.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32.md</td>\n",
       "      <td>4</td>\n",
       "      <td>2696.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20.md</td>\n",
       "      <td>4</td>\n",
       "      <td>3177.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19.md</td>\n",
       "      <td>4</td>\n",
       "      <td>2777.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document  Chunks  Avg. Chunk Length\n",
       "10    11.md       6        3013.833333\n",
       "22    27.md       5        3036.200000\n",
       "28    33.md       5        2749.800000\n",
       "30    35.md       4        2854.750000\n",
       "21    26.md       4        3045.000000\n",
       "19    24.md       4        3408.000000\n",
       "26    31.md       4        2682.000000\n",
       "27    32.md       4        2696.500000\n",
       "16    20.md       4        3177.750000\n",
       "15    19.md       4        2777.250000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "import time\n",
    "\n",
    "# Define a simple progress tracking function that doesn't require tqdm\n",
    "def simple_progress(iterable, desc=\"Processing\"):\n",
    "    \"\"\"A simple progress tracker for notebooks that doesn't require ipywidgets.\"\"\"\n",
    "    total = len(iterable)\n",
    "    start_time = time.time()\n",
    "    print(f\"{desc}: 0/{total} (0%)\")\n",
    "    \n",
    "    for i, item in enumerate(iterable):\n",
    "        yield item\n",
    "        # Update progress every 5% or at least 1 item\n",
    "        if (i+1) % max(1, total//20) == 0 or i+1 == total:\n",
    "            elapsed = time.time() - start_time\n",
    "            percent = (i+1)/total*100\n",
    "            items_per_sec = (i+1)/elapsed if elapsed > 0 else 0\n",
    "            print(f\"{desc}: {i+1}/{total} ({percent:.1f}%) - {items_per_sec:.1f} items/s\")\n",
    "\n",
    "# Define chunking parameters\n",
    "chunk_size = 1200\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Initialize text splitter with our parameters\n",
    "text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Dictionary to store chunks for each document\n",
    "document_chunks = {}\n",
    "\n",
    "# Process each document\n",
    "print(f\"Splitting documents into chunks (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "total_chunks = 0\n",
    "\n",
    "for doc in simple_progress(documents, desc=\"Processing documents\"):\n",
    "    filename = doc.metadata.get('file_name')\n",
    "    # Split the document text into chunks\n",
    "    chunks = text_splitter.split_text(doc.text)\n",
    "    # Store the chunks\n",
    "    document_chunks[filename] = chunks\n",
    "    total_chunks += len(chunks)\n",
    "\n",
    "print(f\"Created {total_chunks} total chunks across {len(documents)} documents\")\n",
    "\n",
    "# Display summary of chunks per document\n",
    "chunk_summary = []\n",
    "for filename, chunks in document_chunks.items():\n",
    "    chunk_summary.append({\n",
    "        'Document': filename,\n",
    "        'Chunks': len(chunks),\n",
    "        'Avg. Chunk Length': sum(len(chunk) for chunk in chunks) / len(chunks) if chunks else 0\n",
    "    })\n",
    "\n",
    "# Create DataFrame and display\n",
    "chunk_df = pd.DataFrame(chunk_summary)\n",
    "display(chunk_df.sort_values(by='Chunks', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6236c47-8584-41d6-8553-f516e282d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from src import get_azure_openai_chat_model, get_azure_openai_mini_model\n",
    "\n",
    "llm1 = get_azure_openai_chat_model()\n",
    "llm2 = get_azure_openai_mini_model()\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "-Goal-\n",
    "Given a text document containing medical protocols or guidelines, identify all entities of the specified medical types and all relationships among the identified entities to build a knowledge graph.\n",
    "\n",
    "-Steps-\n",
    "1.  Identify all entities within the text. For each identified entity, extract the following information:\n",
    "    *   `entity_name`: Name of the entity, capitalized (e.g., CPR, UNCONSCIOUSNESS, NEWBORN).\n",
    "    *   `entity_type`: One of the relevant medical types provided below.\n",
    "    *   `entity_description`: Comprehensive description of the entity's attributes, purpose, or actions as described in the text.\n",
    "    Format each entity as (\"entity\"{{tuple_delimiter}}<entity_name>{{tuple_delimiter}}<entity_type>{{tuple_delimiter}}<entity_description>)\n",
    "\n",
    "2.  From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly and directly related* within the context of the provided text.\n",
    "    For each pair of related entities, extract the following information:\n",
    "    *   `source_entity`: name of the source entity, as identified in step 1.\n",
    "    *   `target_entity`: name of the target entity, as identified in step 1.\n",
    "    *   `relationship_description`: explanation in English describing the nature of the relationship between the source and target entity based *only* on the text (e.g., \"is a symptom of\", \"is used to treat\", \"is performed on\", \"is a type of\", \"is indicated for\", \"uses\", \"should be checked in case of\").\n",
    "    *   `relationship_strength`: an integer score between 1 (weakly related) to 10 (very strongly and explicitly related).\n",
    "    Format each relationship as (\"relationship\"{{tuple_delimiter}}<source_entity>{{tuple_delimiter}}<target_entity>{{tuple_delimiter}}<relationship_description>{{tuple_delimiter}}<relationship_strength>)\n",
    "\n",
    "3.  Return the output as a single list containing all identified entities and relationships. Use **{{record_delimiter}}** as the delimiter between each entity or relationship record. The primary language of the provided text is mixed English and Norwegian.\n",
    "\n",
    "4.  Translate Norwegian descriptions into English for the `entity_description` and `relationship_description` fields *only*. Keep entity names and types consistent (preferably English where obvious equivalents exist, otherwise use the capitalized term from the text).\n",
    "\n",
    "5.  When finished, output {{completion_delimiter}}.\n",
    "\n",
    "-Relevant Medical Entity Types-\n",
    "[Medical Condition, Symptom, Patient Group, Medical Procedure, Medical Device/Tool, Medication/Substance, Anatomical Location, Guideline Section, Organization/Role, Medical Concept]\n",
    "\n",
    "-Examples-\n",
    "######################\n",
    "\n",
    "Example 1 (Based on 01.md/02.md):\n",
    "\n",
    "entity_types: [Medical Condition, Symptom, Patient Group, Medical Procedure, Medical Device/Tool, Medication/Substance, Anatomical Location, Guideline Section, Organization/Role, Medical Concept]\n",
    "text:\n",
    "# 01 Unconscious adult – not breathing normally\n",
    "## CRITERIA\n",
    "- Critical | Unconscious adult, not breathing normally\n",
    "## SITUATIONAL GUIDANCE & IMPORTANT TO ASCERTAIN\n",
    "- Help is on the way as I speak to you.\n",
    "- You must start CPR (reviving the person). I will tell you what to do.\n",
    "- Don’t hang up, put the phone on speaker if you can.\n",
    "- If there is defibrillator at hand, get someone else to fetch it. Check Hjertestarterregisteret (the caller must not fetch a defibrillator / AED if alone)\n",
    "## EMERGENCY RESPONSE\n",
    "### SCENARIO\n",
    "- BCPR (CARDIO PULMONARY RESUSCITATION)\n",
    "### IF YES\n",
    "- Push down at this rate 30 times.\n",
    "- Now give rescue breaths.\n",
    "- Tilt the head back with one hand on the forehead.\n",
    "- Lift the chin up with the other hand.\n",
    "- Pinch the nose and give 2 gentle rescue breaths.\n",
    "- Continue with 30 pushes and 2 rescue breaths until medics take over or the person wakes up.\n",
    "- Lay the person on the floor, on his / her back.\n",
    "- Kneel beside the person’s chest.\n",
    "- Place your hands in the middle of his / her chest... Push down hard...\n",
    "------------------------\n",
    "output:\n",
    "(\"entity\"{{tuple_delimiter}}UNCONSCIOUS ADULT{{tuple_delimiter}}Patient Group{{tuple_delimiter}}An adult patient who is unconscious and not breathing normally, requiring critical intervention.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}NOT BREATHING NORMALLY{{tuple_delimiter}}Symptom{{tuple_delimiter}}A critical symptom indicating lack of normal respiration, often associated with unconsciousness.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}CPR{{tuple_delimiter}}Medical Procedure{{tuple_delimiter}}Cardiopulmonary Resuscitation, a life-saving procedure involving chest compressions and rescue breaths, instructed by the call handler.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}CHEST COMPRESSIONS{{tuple_delimiter}}Medical Procedure{{tuple_delimiter}}A component of CPR involving pushing down hard on the center of the chest 30 times.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}RESCUE BREATHS{{tuple_delimiter}}Medical Procedure{{tuple_delimiter}}A component of CPR involving tilting the head, lifting the chin, pinching the nose, and giving 2 gentle breaths into the mouth.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}AED{{tuple_delimiter}}Medical Device/Tool{{tuple_delimiter}}Automated External Defibrillator (also referred to as defibrillator or Hjertestarter), a device to be fetched if available and if someone else is present.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}CALLER{{tuple_delimiter}}Organization/Role{{tuple_delimiter}}The person calling for help who is instructed to perform CPR.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}MEDICS{{tuple_delimiter}}Organization/Role{{tuple_delimiter}}Emergency medical personnel who are on their way and will take over CPR upon arrival.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}CRITERIA{{tuple_delimiter}}Guideline Section{{tuple_delimiter}}Section defining the conditions under which this protocol applies, such as an unconscious adult not breathing normally.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}CHEST{{tuple_delimiter}}Anatomical Location{{tuple_delimiter}}The location on the body (middle of the chest) where chest compressions are applied during CPR.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}HEAD{{tuple_delimiter}}Anatomical Location{{tuple_delimiter}}Body part manipulated during rescue breaths (tilt the head back).){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}CHIN{{tuple_delimiter}}Anatomical Location{{tuple_delimiter}}Body part manipulated during rescue breaths (lift the chin up).){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}UNCONSCIOUS ADULT{{tuple_delimiter}}NOT BREATHING NORMALLY{{tuple_delimiter}}Is a defining symptom for this patient group according to the criteria.{{tuple_delimiter}}9){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}CPR{{tuple_delimiter}}UNCONSCIOUS ADULT{{tuple_delimiter}}Is the required procedure for an unconscious adult not breathing normally.{{tuple_delimiter}}10){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}CPR{{tuple_delimiter}}CHEST COMPRESSIONS{{tuple_delimiter}}Incorporates chest compressions as a key component (30 pushes).{{tuple_delimiter}}9){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}CPR{{tuple_delimiter}}RESCUE BREATHS{{tuple_delimiter}}Incorporates rescue breaths as a key component (2 breaths).{{tuple_delimiter}}9){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}CALLER{{tuple_delimiter}}CPR{{tuple_delimiter}}Is instructed to perform CPR.{{tuple_delimiter}}8){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}AED{{tuple_delimiter}}CPR{{tuple_delimiter}}Should be fetched and used during CPR if available and feasible.{{tuple_delimiter}}7){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}CHEST COMPRESSIONS{{tuple_delimiter}}CHEST{{tuple_delimiter}}Are applied to the middle of the chest.{{tuple_delimiter}}9){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}RESCUE BREATHS{{tuple_delimiter}}HEAD{{tuple_delimiter}}Involves tilting the head back.{{tuple_delimiter}}8){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}RESCUE BREATHS{{tuple_delimiter}}CHIN{{tuple_delimiter}}Involves lifting the chin up.{{tuple_delimiter}}8){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}MEDICS{{tuple_delimiter}}CPR{{tuple_delimiter}}Will take over CPR from the caller upon arrival.{{tuple_delimiter}}8)\n",
    "{{completion_delimiter}}\n",
    "#############################\n",
    "\n",
    "Example 2 (Based on 20.md):\n",
    "\n",
    "entity_types: [Medical Condition, Symptom, Patient Group, Medical Procedure, Medical Device/Tool, Medication/Substance, Anatomical Location, Guideline Section, Organization/Role, Medical Concept]\n",
    "text:\n",
    "# 20 Diabetes\n",
    "## CRITERIA\n",
    "- Critical | Drowsy (decreased level of consciousness) - May have a low blood sugar (hypo) | 1.2.3.5.6\n",
    "## ADVICE\n",
    "### Advice 5. IF THE PERSON HAS A HYPO\n",
    "If necessary and the person has a glucagon injection or nasal spray:\n",
    "– Give one dose (1 mg) Glucagon...\n",
    "### Advice 7. DROWSY OR DAZED AND UNABLE TO DRINK\n",
    "– Do not force the person to drink...\n",
    "– Alternatively, you can put one or two spoonfuls of honey in the mouth. You can also spread honey or granulated sugar on the gums, between the lips and the teeth.\n",
    "### Advice 8. THE PERSON IS AWAKE ENOUGH TO DRINK\n",
    "– Give the person several glasses of sugary drink e.g. fizzy drink, cordial, juice or milk.\n",
    "## INFORMATION\n",
    "### HYPOGLYKEMI – LAVT BLODSUKKER\n",
    "Når blodsukkeret synker under 4 mmol/l... Hvis blodsukkeret faller ytterligere under 3 mmol/l, opptrer føling (insulinføling)... Pas. kan hurtig bli sløv, bevisstløs eller få kramper. Behandlingen er rask tilførsel av sukker eller Glukagon®\n",
    "------------------------\n",
    "output:\n",
    "(\"entity\"{{tuple_delimiter}}DIABETES{{tuple_delimiter}}Medical Condition{{tuple_delimiter}}The underlying medical condition being addressed in this protocol.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}HYPOGLYCEMIA{{tuple_delimiter}}Medical Condition{{tuple_delimiter}}Low blood sugar (føling/insulinføling), defined as blood sugar below 4 mmol/L or 3 mmol/L, potentially causing drowsiness, unconsciousness, or seizures. Referred to as 'hypo'.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}DROWSINESS{{tuple_delimiter}}Symptom{{tuple_delimiter}}Decreased level of consciousness, listed as a critical criterion possibly indicating hypoglycemia.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}GLUCAGON{{tuple_delimiter}}Medication/Substance{{tuple_delimiter}}A medication administered via injection (1mg or 0.5mg dose) or nasal spray (3mg) to treat severe hypoglycemia if available.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}GLUCAGON KIT{{tuple_delimiter}}Medical Device/Tool{{tuple_delimiter}}Refers to the glucagon injection or nasal spray kit the person might have.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}HONEY{{tuple_delimiter}}Medication/Substance{{tuple_delimiter}}A sugary substance that can be placed in the mouth or on the gums of a drowsy person unable to drink.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}GRANULATED SUGAR{{tuple_delimiter}}Medication/Substance{{tuple_delimiter}}A sugary substance that can be spread on the gums of a drowsy person unable to drink.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}SUGARY DRINK{{tuple_delimiter}}Medication/Substance{{tuple_delimiter}}Drinks like fizzy drinks, cordial, juice, or milk given to a person awake enough to drink to raise blood sugar.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}GUMS{{tuple_delimiter}}Anatomical Location{{tuple_delimiter}}Location in the mouth where honey or granulated sugar can be applied for absorption in a drowsy patient.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}ADVICE{{tuple_delimiter}}Guideline Section{{tuple_delimiter}}Section providing instructions on how to manage specific situations like hypoglycemia.){{record_delimiter}}\n",
    "(\"entity\"{{tuple_delimiter}}INFORMATION{{tuple_delimiter}}Guideline Section{{tuple_delimiter}}Section providing background information on conditions like hypoglycemia.){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}HYPOGLYCEMIA{{tuple_delimiter}}DIABETES{{tuple_delimiter}}Is a potential complication or state related to Diabetes.{{tuple_delimiter}}9){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}DROWSINESS{{tuple_delimiter}}HYPOGLYCEMIA{{tuple_delimiter}}Is listed as a potential symptom or consequence of Hypoglycemia.{{tuple_delimiter}}8){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}GLUCAGON{{tuple_delimiter}}HYPOGLYCEMIA{{tuple_delimiter}}Is a treatment for severe Hypoglycemia.{{tuple_delimiter}}10){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}HONEY{{tuple_delimiter}}HYPOGLYCEMIA{{tuple_delimiter}}Is an alternative treatment for Hypoglycemia in drowsy patients unable to drink.{{tuple_delimiter}}7){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}GRANULATED SUGAR{{tuple_delimiter}}HYPOGLYCEMIA{{tuple_delimiter}}Is an alternative treatment for Hypoglycemia in drowsy patients unable to drink.{{tuple_delimiter}}7){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}SUGARY DRINK{{tuple_delimiter}}HYPOGLYCEMIA{{tuple_delimiter}}Is a treatment for Hypoglycemia in patients awake enough to drink.{{tuple_delimiter}}8){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}GLUCAGON KIT{{tuple_delimiter}}GLUCAGON{{tuple_delimiter}}Is the delivery method for Glucagon medication.{{tuple_delimiter}}9){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}HONEY{{tuple_delimiter}}GUMS{{tuple_delimiter}}Can be applied to the gums for absorption.{{tuple_delimiter}}7){{record_delimiter}}\n",
    "(\"relationship\"{{tuple_delimiter}}GRANULATED SUGAR{{tuple_delimiter}}GUMS{{tuple_delimiter}}Can be applied to the gums for absorption.{{tuple_delimiter}}7)\n",
    "{{completion_delimiter}}\n",
    "#############################\n",
    "\n",
    "-Real Data-\n",
    "######################\n",
    "entity_types: [Medical Condition, Symptom, Patient Group, Medical Procedure, Medical Device/Tool, Medication/Substance, Anatomical Location, Guideline Section, Organization/Role, Medical Concept]\n",
    "text: {input_text}\n",
    "######################\n",
    "output:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = prompt | llm1 | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742c48f-043a-4653-ae9a-550d0b929386",
   "metadata": {},
   "source": [
    "**Creating a Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8e812-85b4-446b-9e54-ca8a227947f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
       "</pre>\n"
      ],
      "text/plain": [
       "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chain.invoke({\"input_text\": documents[0].text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1176f935-4ffb-4e37-8daa-505edced7bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```plaintext\n",
      "(\"entity\"{tuple_delimiter}EVALUATION METRICS{tuple_delimiter}evaluation metrics{tuple_delimiter}Evaluation metrics are used to measure the performance of AI models, including metrics like cross-entropy, perplexity, factuality, and context relevance)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}HYPERPARAMETERS{tuple_delimiter}hyperparameters{tuple_delimiter}Hyperparameters are key settings in model training, such as learning rate, batch size, and number of training epochs, which are adjusted to optimize model performance)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}CROSS-ENTROPY{tuple_delimiter}evaluation metrics{tuple_delimiter}Cross-entropy is a key metric for evaluating large language models (LLMs) during training or fine-tuning, quantifying the difference between predicted and actual data distributions)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}PERPLEXITY{tuple_delimiter}evaluation metrics{tuple_delimiter}Perplexity measures how well a probability distribution or model predicts a sample, indicating the model's uncertainty about the next word in a sequence)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}FACTUALITY{tuple_delimiter}evaluation metrics{tuple_delimiter}Factuality assesses the accuracy of the information produced by the LLM, important for applications where misinformation could have serious consequences)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}LLM UNCERTAINTY{tuple_delimiter}evaluation metrics{tuple_delimiter}LLM uncertainty is measured using log probability to identify low-quality generations, with lower uncertainty indicating higher output quality)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}PROMPT PERPLEXITY{tuple_delimiter}evaluation metrics{tuple_delimiter}Prompt perplexity evaluates how well the model understands the input prompt, with lower values indicating clearer and more comprehensible prompts)\n",
      "{record_delimiter}\n",
      "(\"entity\"{tuple_delimiter}CONTEXT RELEVANCE{tuple_delimiter}evaluation metrics{tuple_delimiter}Context relevance measures how pertinent the retrieved context is to the user query in retrieval-augmented generation systems, improving response quality)\n",
      "{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}CROSS-ENTROPY{tuple_delimiter}PERPLEXITY{tuple_delimiter}Both cross-entropy and perplexity are metrics used to evaluate the performance of large language models, focusing on prediction accuracy and uncertainty{tuple_delimiter}7)\n",
      "{record_delimiter}\n",
      "(\"relationship\"{tuple_delimiter}HYPERPARAMETERS{tuple_delimiter}EVALUATION METRICS{tuple_delimiter}Hyperparameters are adjusted based on evaluation metrics to optimize model performance and prevent overfitting{tuple_delimiter}8)\n",
      "{completion_delimiter}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41d036-00da-4dba-8fcf-1cee5b683d52",
   "metadata": {},
   "source": [
    "### **Looking at Final Entities and Relationships**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "26613855-7891-4b16-ad84-758f8a0ed8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>title</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>text_unit_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e3a7f24b-88b6-4481-b3a7-c35075a9671f</td>\n",
       "      <td>0</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>GPT-3 is a large language model developed by O...</td>\n",
       "      <td>[ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f55cae4e-dd0d-47a2-912b-f7680147dd31</td>\n",
       "      <td>1</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>GPT-4 is an advanced large language model deve...</td>\n",
       "      <td>[ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f3e3e46b-6746-45a7-9a26-1432f14c45e4</td>\n",
       "      <td>2</td>\n",
       "      <td>BERT</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>BERT, which stands for Bidirectional Encoder R...</td>\n",
       "      <td>[ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0491a417-2e18-41c4-ae1e-3e39bf2eb98f</td>\n",
       "      <td>3</td>\n",
       "      <td>PALM</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>PaLM is a large language model developed by Go...</td>\n",
       "      <td>[ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2b7f14f5-d1d5-49f6-bace-46fd1767f99e</td>\n",
       "      <td>4</td>\n",
       "      <td>LLAMA</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>LLAMA is a versatile and advanced model known ...</td>\n",
       "      <td>[ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  title  \\\n",
       "0  e3a7f24b-88b6-4481-b3a7-c35075a9671f                  0  GPT-3   \n",
       "1  f55cae4e-dd0d-47a2-912b-f7680147dd31                  1  GPT-4   \n",
       "2  f3e3e46b-6746-45a7-9a26-1432f14c45e4                  2   BERT   \n",
       "3  0491a417-2e18-41c4-ae1e-3e39bf2eb98f                  3   PALM   \n",
       "4  2b7f14f5-d1d5-49f6-bace-46fd1767f99e                  4  LLAMA   \n",
       "\n",
       "           type                                        description  \\\n",
       "0  ORGANIZATION  GPT-3 is a large language model developed by O...   \n",
       "1  ORGANIZATION  GPT-4 is an advanced large language model deve...   \n",
       "2  ORGANIZATION  BERT, which stands for Bidirectional Encoder R...   \n",
       "3  ORGANIZATION  PaLM is a large language model developed by Go...   \n",
       "4  ORGANIZATION  LLAMA is a versatile and advanced model known ...   \n",
       "\n",
       "                                       text_unit_ids  \n",
       "0  [ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...  \n",
       "1  [ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...  \n",
       "2  [ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...  \n",
       "3  [ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...  \n",
       "4  [ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "entities = pd.read_parquet('./ragtest/output/create_final_entities.parquet')\n",
    "\n",
    "entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4819b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# 01 Unconscious adult – not breathing normally\\n\\n## CRITERIA\\n- Critical | Unconscious adult, not breathing normally\\n\\n## SITUATIONAL GUIDANCE & IMPORTANT TO ASCERTAIN\\n- Help is on the way as I speak to you.\\n- You must start CPR (reviving the person). I will tell you what to do.\\n- Don’t hang up, put the phone on speaker if you can.\\n- If there is defibrillator at hand, get someone else to fetch it. Check Hjertestarterregisteret (the caller must not fetch a defibrillator / AED if alone)\\n- If you suspect a blocked airway, open the mouth to see if you can remove any object.\\n- Are there any children / adolescents present?\\n- Are they in need of immediate care or support?\\n- Er du usikker på om personen puster normalt: Gi veiledning i sikring av fri luftvei og HLR.\\n- Tilby ALLE innringere veiledning, selv om de kan HLR fra før.\\n- Gi kontinuerlig veiledning og oppmuntring.\\n- Dersom innringer tror personen er død, bør HLR-instruksjoner likevel tilbys, såfremt personen ikke har store skader som er uforenelig med liv.\\n- Vær respektfull overfor innringer dersom han/hun mener det ikke er riktig å starte HLR.\\n- Hvis personen er gravid i 3. trimester, legg en pute under høyre hofte / korsryggen på personen så hun blir liggende litt over på venstre side.\\n- Pasienter utsatt for høyspent strøm: Ikke ta i noen som fortsatt er i kontakt med strømkilde.\\n- Se 21 Dødsfall / mulig dødsfall og Legevaktindeks\\n\\n## EMERGENCY RESPONSE\\n\\n### SCENARIO\\n- Overdose, hanging or other reasons related to breathing.\\n- Unconscious adult, not breathing normally\\n- BCPR (CARDIO PULMONARY RESUSCITATION)\\n- AM I (RESCUER) TRAINED IN CPR?\\n\\n### IF YES\\n- Push down at this rate 30 times.\\n- Now give rescue breaths.\\n- Tilt the head back with one hand on the forehead.\\n- Lift the chin up with the other hand.\\n- Pinch the nose and give 2 gentle rescue breaths.\\n- Continue with 30 pushes and 2 rescue breaths until medics take over or the person wakes up.\\n- 30:2 30:2 30:2\\n- Lay the person on the floor, on his / her back.\\n- Kneel beside the person’s chest.\\n- Place your hands in the middle of his / her chest, kneel right by him / her and keep your elbows straight.\\n- Push down hard (with straight elbows) at this rate: 1,2,3,4,5…28,29,30\\n- Count aloud with me\\n\\n### IF NO\\n- Keep pushing at this rate until medics take over or the person wakes up.\\n- Count to 10 and start again if it’s easier.\\n- Lay the person on the floor, on his / her back.\\n- Kneel beside the person’s chest.\\n- Place your hands in the middle of his / her chest, kneel right by him / her and keep your elbows straight.\\n- Push down hard (with straight elbows) at this rate: 1,2,3,4,5…28,29,30\\n- Count aloud with me\\n\\n## EMERGENCY RESPONSE\\n\\n### SCENARIO\\n- Drowning, overdose, hanging or other reasons related to breathing.\\n- IS THE PERSON DROWNING?\\n\\n### IF YES\\n- Start giving breaths as quickly as possible, even before the person is out of the water. \\n- Chest compressions must be started as soon as it is practically possible.\\n- Lay the person on the floor, on his / her back.\\n- Kneel beside the person’s chest.\\n- Place your hands in the middle of his / her chest, kneel right by him / her and keep your elbows straight.\\n- Push down hard (with straight elbows) at this rate: 1,2,3,4,5…28,29,30\\n- Count aloud with me\\n\\n\\n## EMERGENCY RESPONSE\\n\\n### SCENARIO\\n- IS THERE AN AED AT HAND?\\n\\n### IF YES\\n- If there is a defibrillator at hand, get someone else to fetch it.\\n- Follow the instructions – the defibrillator will tell you what to do.\\n- Continue pushing the chest until the defibrillator tells you to take your hands off the person.\\n\\n\\n## How to give good compressions: \\n- Push down hard and deep, straight elbows,\\nuse your body weight.\\n- Push down about 5 cms. at the rate of 100\\nper minute, release completely between each compression.\\n- Count aloud with me: 1, 2, 3, 4, 5 ... 28, 29, 30.\\n- You are doing',\n",
       " ' Continue pushing the chest until the defibrillator tells you to take your hands off the person.\\n\\n\\n## How to give good compressions: \\n- Push down hard and deep, straight elbows,\\nuse your body weight.\\n- Push down about 5 cms. at the rate of 100\\nper minute, release completely between each compression.\\n- Count aloud with me: 1, 2, 3, 4, 5 ... 28, 29, 30.\\n- You are doing really well. Keep going until the medics take over.\\n\\n## VED 30:2 How to establish a free airway:\\n- Does the chest rise when you give breaths?\\n- Tilt the head back with one hand on the forehead.\\n- Lift the chin up with the other hand.\\n- Give 2 gentle breaths.\\nIf you suspect a blocked airway:\\n- Open the mouth to see if you can remove any object.\\n- If it isn’t possible to ventilate the person, consider just giving chest compressions.\\n\\n\\n## BEVISSTLØS, PUSTER IKKE NORMALT\\nNøkkelen til god overlevelse etter hjertestans er tidlig varsling, tidlig hjerte-lungeredning\\n(HLR) og tidlig defibrillering. Etter at egensirkulasjonen er gjenopprettet (ROSC), er\\nsystematisk intensivbehandling og mulighet for å identifisere og behandle koronariskemi\\nviktig.\\n\\n## TIDLIG VARSLING\\nTidlig varsling er avhengig av at innringer erkjenner at situasjonen er alvorlig og vet at\\nde skal ringe 113. Hvis vi får satt over telefon fra LVS eller annen operasjonssentral,\\ner det viktig å prøve å komme i direkte kontakt med de som er hos pasienten. Ofte\\nbruker innringer ord som «falt om», «kollapset», «besvimt» eller «reagerer ikke», tidlig\\ni samtalen og da må vi tidlig mistenke hjertestans. Noen innringere kan raskt fortelle\\nat det dreier seg om hjertestans og at de allerede har startet HLR. Da må vi iverksette\\nvår varsling, men også forsikre oss om at HLR blir gjort best mulig. Andre innringere er\\nusikre og vi må lede dem gjennom undersøkelse av bevissthet og pust på startkortet.\\nUsikkerhet om agonal pust er den viktigste enkelt årsak til forsinket gjenkjennelse av\\nhjertestans. Agonal pust beskrives som «snorking», «gisping» eller «puster tungt», men\\nvil oftest være uregelmessig og annerledes enn normal pust. Det er viktig å informere\\nom at ambulanse er på vei, selv om vi fortsetter veiledningen.\\n\\n## TIDLIG HLR\\nTidlig HLR kan redde liv, men mange er usikre på hvordan det skal gjøres og redde for\\nskade. Vi vil derfor gi instruksjoner, veiledning og oppmuntring til alle innringere. Innringere\\nsom vet hvordan de kan bruke høyttalerfunksjonen, kan få veiledning og oppmuntring\\nmens de gjør brystkompresjoner. Still kontrollspørsmål for å sikre at kvaliteten på\\nbrystkompresjonene blir god (få pas. på gulvet/ bakken, legg ene armen rett ut fra\\nkroppen og sitt på kne med ett ben på hver side av den utstrakte armen, trykk midt på\\nbrystet, bruk strake armer og hele kroppstyngden, trykk hardt, tell høyt sammen med\\nmeg for å holde riktig takt, unngå pauser). En metronom som klikker i takt, kan hjelpe\\nbåde innringer og operatør med å holde takten på brystkompresjonene (100–120/min).\\nFor de som ikke er trent i HLR, skal vi bare instruere i brystkompresjoner. For de som\\nkan HLR med kompresjoner og ventilasjoner, må vi stille kontrollspørsmål for å sikre at\\nluftveiene er frie, at brystet hever seg og at pausene holdes s',\n",
       " 'For de som ikke er trent i HLR, skal vi bare instruere i brystkompresjoner. For de som\\nkan HLR med kompresjoner og ventilasjoner, må vi stille kontrollspørsmål for å sikre at\\nluftveiene er frie, at brystet hever seg og at pausene holdes så korte som mulig.\\n\\n## TIDLIG DEFIBRILLERING\\nTidlig defibrillering er viktig for den fjerdedelen av pasientene som har en sjokkbar rytme,\\nmen vi kan ikke vite hvem det er før det er koblet til en defibrillator. Derfor skal vi prøve å\\nfå en defibrillator (hjertestarter) til alle pasienter med mistenkt hjertestans så tidlig som\\nmulig. Tidlig respons med ambulanse og eventuelt akutthjelper (brann, politi, frivillige)\\ner viktigst. Dersom det er mer enn én livredder og det finnes en offentlig tilgjengelig\\nhjertestarter i nærheten (Hjertestarterregisteret), må du vurdere om én av livredderne\\nkan løpe og hente hjertestarteren uten at kvaliteten på HLR forringes.\\n\\n\\n## ETTER ROSC\\nEtter ROSC er det viktig at pas. kan komme til et sykehus med mulighet for utredning\\nog behandling av iskemisk hjertesykdom. Pasientene kan være svært ustabile og\\nanestesilegeassistanse og/eller transport til nærmeste sykehus for stabilisering må\\nvurderes.\\n\\n## OVERLEVELSE VED DRUKNING\\nDette avhenger først og fremst av hvor lenge hjernen har vært uten oksygen. Raskt\\nigangsatt ventilasjon (munn-til-munn) og brystkompresjon kan alene starte sirkulasjonen\\n(i motsetning til ved ventrikkelflimmer i forbindelse med akutt hjertesykdom der\\ndefibrillering er helt nødvendig). Derfor anbefales hurtigst mulig start av innblåsinger,\\nhelst før pas. er oppe av vannet, og så at brystkompresjon startes så snart det er praktisk\\nmulig.\\n\\n\\n## AVSTÅ FRA Å STARTE HLR\\nDette kan vi bare gjøre der det per telefon kan fastslås at pas. med sikkerhet er død\\n(skader uforenelig med liv, dødsstivhet i fravær av aksidentell hypotermi, begynnende\\nforråtnelse osv.) eller der det kommer frem at pas. er terminalt syk og ikke ønsker forsøk\\npå gjenopplivning (HLR minus). Der vi er i tvil om observasjonene eller gyldigheten av\\nslike opplysninger, må vi veilede innringer som vanlig i påvente av at egne ressurser\\nankommer og kan revurdere situasjonen.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5685f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "entities = pd.read_parquet('./ragtest/output/create_final_entities.parquet')\n",
    "\n",
    "entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c4bf9fc-bb4e-4546-897b-991236079323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>description</th>\n",
       "      <th>weight</th>\n",
       "      <th>combined_degree</th>\n",
       "      <th>text_unit_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b895553a-f860-4d15-bba2-a42f1464e810</td>\n",
       "      <td>0</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>GPT-4 is an advanced version of GPT-3, buildin...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20</td>\n",
       "      <td>[ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1548feb2-5a6a-43e6-ab44-81252056193e</td>\n",
       "      <td>1</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>CHATGPT</td>\n",
       "      <td>ChatGPT is based on the GPT architecture, spec...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14</td>\n",
       "      <td>[ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e538721d-c023-4994-b918-1efece80ea7e</td>\n",
       "      <td>2</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>BERT</td>\n",
       "      <td>Both BERT and GPT-3 are pre-trained language m...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18</td>\n",
       "      <td>[ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>063a2941-df53-4b5c-a66a-45e60bdba604</td>\n",
       "      <td>3</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>REINFORCEMENT LEARNING FROM HUMAN FEEDBACK (RLHF)</td>\n",
       "      <td>RLHF is used in training GPT-3 to refine its o...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13</td>\n",
       "      <td>[ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7e0cd0b4-4688-434f-a194-f2b9ce397a94</td>\n",
       "      <td>4</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>PROMPT ENGINEERING</td>\n",
       "      <td>Prompt engineering is a technique used to guid...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14</td>\n",
       "      <td>[ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id source  \\\n",
       "0  b895553a-f860-4d15-bba2-a42f1464e810                  0  GPT-3   \n",
       "1  1548feb2-5a6a-43e6-ab44-81252056193e                  1  GPT-3   \n",
       "2  e538721d-c023-4994-b918-1efece80ea7e                  2  GPT-3   \n",
       "3  063a2941-df53-4b5c-a66a-45e60bdba604                  3  GPT-3   \n",
       "4  7e0cd0b4-4688-434f-a194-f2b9ce397a94                  4  GPT-3   \n",
       "\n",
       "                                              target  \\\n",
       "0                                              GPT-4   \n",
       "1                                            CHATGPT   \n",
       "2                                               BERT   \n",
       "3  REINFORCEMENT LEARNING FROM HUMAN FEEDBACK (RLHF)   \n",
       "4                                 PROMPT ENGINEERING   \n",
       "\n",
       "                                         description  weight  combined_degree  \\\n",
       "0  GPT-4 is an advanced version of GPT-3, buildin...     8.0               20   \n",
       "1  ChatGPT is based on the GPT architecture, spec...     7.0               14   \n",
       "2  Both BERT and GPT-3 are pre-trained language m...     6.0               18   \n",
       "3  RLHF is used in training GPT-3 to refine its o...     7.0               13   \n",
       "4  Prompt engineering is a technique used to guid...     6.0               14   \n",
       "\n",
       "                                       text_unit_ids  \n",
       "0  [ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...  \n",
       "1  [ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...  \n",
       "2  [ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...  \n",
       "3  [ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...  \n",
       "4  [ca73c495111f5cadd87e6a7a01aed66647ae6623fdf41...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationships = pd.read_parquet('./ragtest/output/create_final_relationships.parquet')\n",
    "\n",
    "relationships.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c77836-f9ae-4602-a4d0-760915248e0a",
   "metadata": {},
   "source": [
    "### **Community Detection & Node Embedding**\n",
    "\n",
    "<img src=\"./media/leidan.png\" width=600>\n",
    "\n",
    "After we have our basic graph with entities and relationships, we analyze its structure in two ways. Community Detection uses the [Leiden algorithm](https://en.wikipedia.org/wiki/Leiden_algorithm) to find explicit groupings in the graph, creating a hierarchy of related entities. The lower in the hierarchy, the more granular the community. Node Embedding uses [Node2Vec](https://arxiv.org/abs/1607.00653) to create vector representations of each entity, capturing implicit relationships in the graph structure. These complementary approaches let us understand both obvious connections through communities and subtle patterns through embeddings.\n",
    "\n",
    "Combining all of this with our relationships gives us our final nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "058d8f6d-6eb7-45fe-bf1c-e28055a683c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>title</th>\n",
       "      <th>community</th>\n",
       "      <th>level</th>\n",
       "      <th>degree</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e3a7f24b-88b6-4481-b3a7-c35075a9671f</td>\n",
       "      <td>0</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>-4.875545</td>\n",
       "      <td>4.017587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e3a7f24b-88b6-4481-b3a7-c35075a9671f</td>\n",
       "      <td>0</td>\n",
       "      <td>GPT-3</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>-4.875545</td>\n",
       "      <td>4.017587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f55cae4e-dd0d-47a2-912b-f7680147dd31</td>\n",
       "      <td>1</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.561064</td>\n",
       "      <td>1.505724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f55cae4e-dd0d-47a2-912b-f7680147dd31</td>\n",
       "      <td>1</td>\n",
       "      <td>GPT-4</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.561064</td>\n",
       "      <td>1.505724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f3e3e46b-6746-45a7-9a26-1432f14c45e4</td>\n",
       "      <td>2</td>\n",
       "      <td>BERT</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-5.710580</td>\n",
       "      <td>3.546957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f3e3e46b-6746-45a7-9a26-1432f14c45e4</td>\n",
       "      <td>2</td>\n",
       "      <td>BERT</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>-5.710580</td>\n",
       "      <td>3.546957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0491a417-2e18-41c4-ae1e-3e39bf2eb98f</td>\n",
       "      <td>3</td>\n",
       "      <td>PALM</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-5.309392</td>\n",
       "      <td>1.548029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0491a417-2e18-41c4-ae1e-3e39bf2eb98f</td>\n",
       "      <td>3</td>\n",
       "      <td>PALM</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-5.309392</td>\n",
       "      <td>1.548029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2b7f14f5-d1d5-49f6-bace-46fd1767f99e</td>\n",
       "      <td>4</td>\n",
       "      <td>LLAMA</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.644573</td>\n",
       "      <td>0.421999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2b7f14f5-d1d5-49f6-bace-46fd1767f99e</td>\n",
       "      <td>4</td>\n",
       "      <td>LLAMA</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.644573</td>\n",
       "      <td>0.421999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  title  community  \\\n",
       "0  e3a7f24b-88b6-4481-b3a7-c35075a9671f                  0  GPT-3          8   \n",
       "1  e3a7f24b-88b6-4481-b3a7-c35075a9671f                  0  GPT-3         43   \n",
       "2  f55cae4e-dd0d-47a2-912b-f7680147dd31                  1  GPT-4          8   \n",
       "3  f55cae4e-dd0d-47a2-912b-f7680147dd31                  1  GPT-4         46   \n",
       "4  f3e3e46b-6746-45a7-9a26-1432f14c45e4                  2   BERT          8   \n",
       "5  f3e3e46b-6746-45a7-9a26-1432f14c45e4                  2   BERT         44   \n",
       "6  0491a417-2e18-41c4-ae1e-3e39bf2eb98f                  3   PALM          8   \n",
       "7  0491a417-2e18-41c4-ae1e-3e39bf2eb98f                  3   PALM         46   \n",
       "8  2b7f14f5-d1d5-49f6-bace-46fd1767f99e                  4  LLAMA          3   \n",
       "9  2b7f14f5-d1d5-49f6-bace-46fd1767f99e                  4  LLAMA         27   \n",
       "\n",
       "   level  degree         x         y  \n",
       "0      0      12 -4.875545  4.017587  \n",
       "1      1      12 -4.875545  4.017587  \n",
       "2      0       8 -4.561064  1.505724  \n",
       "3      1       8 -4.561064  1.505724  \n",
       "4      0       6 -5.710580  3.546957  \n",
       "5      1       6 -5.710580  3.546957  \n",
       "6      0       3 -5.309392  1.548029  \n",
       "7      1       3 -5.309392  1.548029  \n",
       "8      0       4 -6.644573  0.421999  \n",
       "9      1       4 -6.644573  0.421999  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = pd.read_parquet('./ragtest/output/create_final_nodes.parquet')\n",
    "\n",
    "nodes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b12c9-fc5c-47cf-a597-6c6bfb3177ee",
   "metadata": {},
   "source": [
    "At this step the graph is effectively created, however we can introduce a few extra steps that will allow us to do some advanced retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8ed95-677b-405e-82d3-6fb1a3f1917c",
   "metadata": {},
   "source": [
    "### Community Report Generation & Summarization\n",
    "\n",
    "Now that we have clear community grouping, we can aggregate the main concepts across hierarchical node communities with another generation step, and a shorthand summary of that summary. Similar to the nodes, these summaries are also ran through an embedding model and stored in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "702e5559-ed4e-4964-bf72-113912974102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>community</th>\n",
       "      <th>parent</th>\n",
       "      <th>level</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>full_content</th>\n",
       "      <th>rank</th>\n",
       "      <th>rank_explanation</th>\n",
       "      <th>findings</th>\n",
       "      <th>full_content_json</th>\n",
       "      <th>period</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a85d59a64a054114982b1ce6e1ced591</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>Amazon Bedrock and AI Model Providers</td>\n",
       "      <td>The community is centered around Amazon Bedroc...</td>\n",
       "      <td># Amazon Bedrock and AI Model Providers\\n\\nThe...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>The impact severity rating is high due to Amaz...</td>\n",
       "      <td>[{'explanation': 'Amazon Bedrock is a pivotal ...</td>\n",
       "      <td>{\\n    \"title\": \"Amazon Bedrock and AI Model P...</td>\n",
       "      <td>2024-12-18</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6aafc6eeddd848bc8ffbfb9177790c26</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>AWS and SageMaker JumpStart</td>\n",
       "      <td>The community is centered around Amazon Web Se...</td>\n",
       "      <td># AWS and SageMaker JumpStart\\n\\nThe community...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>The impact severity rating is high due to AWS'...</td>\n",
       "      <td>[{'explanation': 'Amazon Web Services (AWS) is...</td>\n",
       "      <td>{\\n    \"title\": \"AWS and SageMaker JumpStart\",...</td>\n",
       "      <td>2024-12-18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e13e3ed0a0b74fd090319957ae9f3e1e</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PPO for LLM Alignment and Reinforcement Learni...</td>\n",
       "      <td>The community centers around the study 'PPO fo...</td>\n",
       "      <td># PPO for LLM Alignment and Reinforcement Lear...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'The study 'PPO for LLM Align...</td>\n",
       "      <td>{\\n    \"title\": \"PPO for LLM Alignment and Rei...</td>\n",
       "      <td>2024-12-18</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>828baab1461b439ea71203ad8fd0aae5</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>HuggingFace and Advanced NLP Tools</td>\n",
       "      <td>The community is centered around HuggingFace, ...</td>\n",
       "      <td># HuggingFace and Advanced NLP Tools\\n\\nThe co...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>The impact severity rating is high due to Hugg...</td>\n",
       "      <td>[{'explanation': 'HuggingFace is a prominent e...</td>\n",
       "      <td>{\\n    \"title\": \"HuggingFace and Advanced NLP ...</td>\n",
       "      <td>2024-12-18</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>791da6e7031e45228442b277e7d912c6</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>OpenAI and AI Development Platforms</td>\n",
       "      <td>The community is centered around OpenAI, a lea...</td>\n",
       "      <td># OpenAI and AI Development Platforms\\n\\nThe c...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'OpenAI is a central entity i...</td>\n",
       "      <td>{\\n    \"title\": \"OpenAI and AI Development Pla...</td>\n",
       "      <td>2024-12-18</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  human_readable_id  community  parent  \\\n",
       "0  a85d59a64a054114982b1ce6e1ced591                 61         61      32   \n",
       "1  6aafc6eeddd848bc8ffbfb9177790c26                 62         62      32   \n",
       "2  e13e3ed0a0b74fd090319957ae9f3e1e                 14         14       0   \n",
       "3  828baab1461b439ea71203ad8fd0aae5                 15         15       0   \n",
       "4  791da6e7031e45228442b277e7d912c6                 16         16       0   \n",
       "\n",
       "   level                                              title  \\\n",
       "0      2              Amazon Bedrock and AI Model Providers   \n",
       "1      2                        AWS and SageMaker JumpStart   \n",
       "2      1  PPO for LLM Alignment and Reinforcement Learni...   \n",
       "3      1                 HuggingFace and Advanced NLP Tools   \n",
       "4      1                OpenAI and AI Development Platforms   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The community is centered around Amazon Bedroc...   \n",
       "1  The community is centered around Amazon Web Se...   \n",
       "2  The community centers around the study 'PPO fo...   \n",
       "3  The community is centered around HuggingFace, ...   \n",
       "4  The community is centered around OpenAI, a lea...   \n",
       "\n",
       "                                        full_content  rank  \\\n",
       "0  # Amazon Bedrock and AI Model Providers\\n\\nThe...   8.5   \n",
       "1  # AWS and SageMaker JumpStart\\n\\nThe community...   8.5   \n",
       "2  # PPO for LLM Alignment and Reinforcement Lear...   7.5   \n",
       "3  # HuggingFace and Advanced NLP Tools\\n\\nThe co...   8.5   \n",
       "4  # OpenAI and AI Development Platforms\\n\\nThe c...   8.5   \n",
       "\n",
       "                                    rank_explanation  \\\n",
       "0  The impact severity rating is high due to Amaz...   \n",
       "1  The impact severity rating is high due to AWS'...   \n",
       "2  The impact severity rating is high due to the ...   \n",
       "3  The impact severity rating is high due to Hugg...   \n",
       "4  The impact severity rating is high due to the ...   \n",
       "\n",
       "                                            findings  \\\n",
       "0  [{'explanation': 'Amazon Bedrock is a pivotal ...   \n",
       "1  [{'explanation': 'Amazon Web Services (AWS) is...   \n",
       "2  [{'explanation': 'The study 'PPO for LLM Align...   \n",
       "3  [{'explanation': 'HuggingFace is a prominent e...   \n",
       "4  [{'explanation': 'OpenAI is a central entity i...   \n",
       "\n",
       "                                   full_content_json      period  size  \n",
       "0  {\\n    \"title\": \"Amazon Bedrock and AI Model P...  2024-12-18     9  \n",
       "1  {\\n    \"title\": \"AWS and SageMaker JumpStart\",...  2024-12-18     2  \n",
       "2  {\\n    \"title\": \"PPO for LLM Alignment and Rei...  2024-12-18     7  \n",
       "3  {\\n    \"title\": \"HuggingFace and Advanced NLP ...  2024-12-18     7  \n",
       "4  {\\n    \"title\": \"OpenAI and AI Development Pla...  2024-12-18     7  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_reports = pd.read_parquet('./ragtest/output/create_final_community_reports.parquet')\n",
    "\n",
    "community_reports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ca51c33-92db-47c1-9f00-0a8e934332fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Amazon Bedrock and AI Model Providers\n",
      "\n",
      "The community is centered around Amazon Bedrock, a service by AWS that facilitates access to foundation models from various AI innovators. Key entities include AI21 Labs, Anthropic, Cohere, Mistral AI, and Stability AI, all of which provide models accessible through Amazon Bedrock. The service integrates with AWS infrastructure, including AWS Lambda and AWS SageMaker, to support scalable AI model deployment.\n",
      "\n",
      "## Amazon Bedrock as a central service\n",
      "\n",
      "Amazon Bedrock is a pivotal service within the AWS ecosystem, designed to simplify access to high-performing foundation models for generative AI applications. It integrates seamlessly with other AWS services, such as Amazon S3, AWS Lambda, and AWS SageMaker, to facilitate the fine-tuning and deployment of AI models. This integration underscores its importance in the AI landscape, providing a comprehensive suite of tools for scalable AI model deployment [Data: Entities (206); Relationships (281, 326, 327)].\n",
      "\n",
      "## AI21 Labs' contribution to Amazon Bedrock\n",
      "\n",
      "AI21 Labs is one of the key providers of foundation models available through Amazon Bedrock. The company's advanced AI models contribute significantly to the capabilities offered by Amazon Bedrock, enhancing its utility for natural language processing tasks. This partnership highlights the collaborative nature of the AI community in advancing technology through shared resources and expertise [Data: Entities (267); Relationships (318)].\n",
      "\n",
      "## Anthropic's role in AI safety and research\n",
      "\n",
      "Anthropic is an AI safety and research company that provides foundation models accessible through Amazon Bedrock. Its focus on AI safety is crucial in the development of responsible AI technologies, ensuring that the models deployed via Amazon Bedrock adhere to ethical standards. This relationship emphasizes the importance of integrating safety considerations into AI development [Data: Entities (268); Relationships (319)].\n",
      "\n",
      "## Cohere's NLP models in Amazon Bedrock\n",
      "\n",
      "Cohere offers natural language processing models that are part of the foundation models available through Amazon Bedrock. These models enhance the service's capabilities in processing and understanding human language, making it a valuable tool for businesses and developers seeking to implement NLP solutions. Cohere's involvement underscores the diversity of AI models supported by Amazon Bedrock [Data: Entities (269); Relationships (320)].\n",
      "\n",
      "## Integration with AWS Lambda and AWS SageMaker\n",
      "\n",
      "Amazon Bedrock's integration with AWS Lambda and AWS SageMaker is a key feature that supports the deployment and management of AI models. AWS Lambda provides serverless computing capabilities, allowing for efficient resource management, while AWS SageMaker offers tools for building, training, and deploying machine learning models. This integration facilitates a seamless workflow for AI model development and deployment, enhancing the overall efficiency and scalability of AI applications [Data: Entities (276, 277); Relationships (326, 327)].\n"
     ]
    }
   ],
   "source": [
    "print(community_reports[\"full_content\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42c3e76b-e59f-4577-b1a0-3b07f0ebb499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The community is centered around Amazon Bedrock, a service by AWS that facilitates access to foundation models from various AI innovators. Key entities include AI21 Labs, Anthropic, Cohere, Mistral AI, and Stability AI, all of which provide models accessible through Amazon Bedrock. The service integrates with AWS infrastructure, including AWS Lambda and AWS SageMaker, to support scalable AI model deployment.\n"
     ]
    }
   ],
   "source": [
    "print(community_reports[\"summary\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5243d1-e1d2-4525-ab3c-e1457db2eea7",
   "metadata": {},
   "source": [
    "### The Final Graph!\n",
    "\n",
    "<img src=\"./media/ghraphrag_viz.svg\" width=800>\n",
    "\n",
    "*[Full Size PDF](./ghraphrag_viz.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bf775-ee3b-4d4a-a973-317f1681b8af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GraphRAG Retrieval\n",
    "\n",
    "<img src=\"./media/kg_retrieval.png\" width=600>\n",
    "\n",
    "*[Unifying Large Language Models and Knowledge Graphs: A Roadmap](https://arxiv.org/pdf/2306.08302)*\n",
    "\n",
    "With our knowledge graph constructed, and hierarchichal communities delineated, we can now perform multiple types of search that can both take advantage of the graph structure, and multiple levels of specificity across our communities. Specifically:\n",
    "\n",
    "1. **Global Search**: Uses the LLM Generated community reports from a specified level of the graph's community hierarchy as context data to generate response.\n",
    "2. **Local Search**: Combines structured data from the knowledge graph with unstructured data from the input document(s) to augment the LLM context with relevant entity information.\n",
    "3. **Drift Search**: Dynamic Reasoning and Inference with Flexible Traversal, an approach to local search queries by including community information in the search process, thus combining global and local search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a5823-048b-4a54-9da5-51305b8a1c7a",
   "metadata": {},
   "source": [
    "**GraphRAG Retrieval Function**\n",
    "\n",
    "*Note: Wrapping the [GraphRAG CLI tool](https://microsoft.github.io/graphrag/cli/) as a function here instead of using their [library](https://microsoft.github.io/graphrag/examples_notebooks/api_overview/) for an easier example. As such, notebook needs to be running in the same GraphRAG environment/kernal.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ae01778e-518f-42fe-929e-dd2ef63a8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "from typing import Optional\n",
    "\n",
    "def query_graphrag(\n",
    "    query: str,\n",
    "    method: str = \"global\",\n",
    "    root_path: str = \"./ragtest\",\n",
    "    timeout: Optional[int] = None,\n",
    "    community_level: int = 2,\n",
    "    dynamic_community_selection: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Execute a GraphRAG query using the CLI tool.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query string to process\n",
    "        method (str): Query method (e.g., \"global\", \"local\", or \"drift\")\n",
    "        root_path (str): Path to the root directory\n",
    "        timeout (int, optional): Timeout in seconds for the command\n",
    "        community_level (int): The community level in the Leiden community hierarchy (default: 2)\n",
    "        dynamic_community_selection (bool): Whether to use global search with dynamic community selection (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        str: The output from GraphRAG\n",
    "        \n",
    "    Raises:\n",
    "        subprocess.CalledProcessError: If the command fails\n",
    "        subprocess.TimeoutExpired: If the command times out\n",
    "        ValueError: If community_level is negative\n",
    "    \"\"\"\n",
    "    # Validate community level\n",
    "    if community_level < 0:\n",
    "        raise ValueError(\"Community level must be non-negative\")\n",
    "    \n",
    "    # Construct the base command\n",
    "    command = [\n",
    "        'graphrag', 'query',\n",
    "        '--root', root_path,\n",
    "        '--method', method,\n",
    "        '--query', query,\n",
    "        '--community-level', str(community_level)\n",
    "    ]\n",
    "    \n",
    "    # Add dynamic community selection flag if enabled\n",
    "    if dynamic_community_selection:\n",
    "        command.append('--dynamic-community-selection')\n",
    "    \n",
    "    try:\n",
    "        # Execute the command and capture output\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        \n",
    "        # Check if the command was successful\n",
    "        result.check_returncode()\n",
    "        \n",
    "        return result.stdout.strip()\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        error_message = f\"Command failed with exit code {e.returncode}\\nError: {e.stderr}\"\n",
    "        raise subprocess.CalledProcessError(\n",
    "            e.returncode,\n",
    "            e.cmd,\n",
    "            output=e.output,\n",
    "            stderr=error_message\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02da6ea-e0ad-41d4-982a-f9058184add0",
   "metadata": {},
   "source": [
    "### Local Search\n",
    "\n",
    "<img src=\"./media/local_search.png\" width=900>\n",
    "\n",
    "The GraphRAG approach to local search is the most similar to regular semantic RAG search. It combines structured data from the knowledge graph with unstructured data from the input documents to augment the LLM context with relevant entity information. In essence, we are going to first search for relevant entities to the query using semantic search. These become the entry points on our graph that we can now traverse. Starting at these points, we look at connected chunks of text, community reports, other entities, and relationships between them. All of the data retrieved is filtered and ranked to fit into a pre-defined context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ad477f20-1019-4da6-a70a-9cc9e362fb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result:\n",
      "INFO: Vector Store Args: {\n",
      "    \"type\": \"lancedb\",\n",
      "    \"db_uri\": \"/Users/adamlucek/Desktop/github/GraphRAG/ragtest/output/lancedb\",\n",
      "    \"container_name\": \"==== REDACTED ====\",\n",
      "    \"overwrite\": true\n",
      "}\n",
      "creating llm client with {'api_key': 'REDACTED,len=51', 'type': \"openai_chat\", 'encoding_model': 'cl100k_base', 'model': 'gpt-4o', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n",
      "creating embedding llm client with {'api_key': 'REDACTED,len=51', 'type': \"openai_embedding\", 'encoding_model': 'cl100k_base', 'model': 'text-embedding-3-small', 'max_tokens': 4000, 'temperature': 0, 'top_p': 1, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': None, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n",
      "\n",
      "SUCCESS: Local Search Response:\n",
      "When a company is deciding between Retrieval-Augmented Generation (RAG), fine-tuning, and various Parameter-Efficient Fine-Tuning (PEFT) approaches, several factors must be considered, including the specific application requirements, available resources, and desired outcomes. Each method offers distinct advantages and is suited to different scenarios.\n",
      "\n",
      "### Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "RAG is particularly beneficial for applications that require integrating external data to enhance the accuracy and relevance of generated content. It is ideal for scenarios where the model needs to access up-to-date or domain-specific information without extensive retraining. For instance, RAG is effectively used in question-and-answer systems, customer support automation, and summarization tasks, where the ability to retrieve and incorporate external data can significantly improve performance [Data: Reports (20); Entities (20); Relationships (14, 21, 22, 23)]. Companies might choose RAG when they need to maintain high precision and adaptability in dynamic environments, such as customer service or technical support, where the context and information are constantly evolving.\n",
      "\n",
      "### Fine-Tuning\n",
      "\n",
      "Fine-tuning involves adapting a pre-trained model to perform better on specific tasks by updating its parameters with domain-specific data. This approach is essential when a company needs to tailor a model to a particular application or domain, ensuring that it can handle specific tasks with high accuracy. Fine-tuning is a comprehensive process that includes stages such as dataset preparation, model initialization, and evaluation, making it suitable for applications where precise model adaptation is crucial [Data: Reports (19); Entities (17); Relationships (10, 12, 24, 27, 29, 274, 275, 276, 277); Sources (3, 12, 13)]. Companies might opt for fine-tuning when they have the resources to manage the computational demands and when the task requires a high degree of customization.\n",
      "\n",
      "### Parameter-Efficient Fine-Tuning (PEFT)\n",
      "\n",
      "PEFT methods, such as LoRA and QLoRA, offer a more resource-efficient alternative to traditional fine-tuning by updating only a small subset of model parameters. This approach is particularly advantageous in scenarios where computational resources are limited, as it reduces the memory and processing requirements while maintaining performance levels comparable to full fine-tuning [Data: Entities (25, 104, 110, 130); Relationships (24, 137, 140, 104); Sources (6, 15)]. PEFT is suitable for companies looking to optimize the financial and environmental costs associated with fine-tuning large language models, especially in low-data scenarios or when deploying models on less powerful hardware.\n",
      "\n",
      "### Decision-Making Considerations\n",
      "\n",
      "In choosing between these methods, companies should evaluate the specific needs of their application, the availability of computational resources, and the importance of model adaptability and precision. RAG is ideal for applications requiring real-time data integration, fine-tuning is best for highly customized tasks, and PEFT is suitable for resource-constrained environments. By aligning the choice of method with their strategic goals and operational constraints, companies can effectively enhance their AI capabilities and achieve desired outcomes.\n"
     ]
    }
   ],
   "source": [
    "result = query_graphrag(\n",
    "    query=\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\",\n",
    "    method=\"local\"\n",
    ")\n",
    "print(\"Query result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8368cd-25bb-4460-802f-8ed76a39bb2d",
   "metadata": {},
   "source": [
    "### Global Search\n",
    "\n",
    "<img src=\"./media/global_search.png\" width=1000>\n",
    "\n",
    "Through the semantic clustering of communities during the indexxing process outlined above we created community reports as summaries of high level themes across these groupings. Having this community summary data at various levels allows us to do something that traditional RAG performs poorly at, answering queries about broad themes and ideas across our unstructured data.\n",
    "\n",
    "To capture as much broad information as possible in an efficient manner, GraphRAG implements a [map reduce](https://en.wikipedia.org/wiki/MapReduce) approach. Given a query, relevant community node reports at a specific hierarchical level are retrieved. These are shuffled and chunked, where each chunk is used to generate a list of points that each have their own \"importance score\". These intermediate points are ranked and filtered, attempting to maintain the most important points. These become the aggregate intermediary response, which is passed to the LLM as the context for the final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "77b59560-c02f-4c2e-a969-8042caef03bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result:\n",
      "creating llm client with {'api_key': 'REDACTED,len=51', 'type': \"openai_chat\", 'encoding_model': 'cl100k_base', 'model': 'gpt-4o', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n",
      "\n",
      "SUCCESS: Global Search Response:\n",
      "### Choosing Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When a company is deciding between Retrieval-Augmented Generation (RAG), fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) approaches, several key factors must be considered. These factors include the specific requirements of the application, the need for external data integration, computational resource constraints, and the desired level of model adaptation and performance.\n",
      "\n",
      "#### Application Requirements\n",
      "\n",
      "The choice largely depends on the specific needs of the application. For instance, if the application requires high precision and adaptability, such as in customer support or information retrieval systems, RAG may be the preferred choice. RAG enhances large language models by integrating external data, allowing for more accurate and contextually relevant content generation without extensive fine-tuning [Data: Reports (19, 20, 35, 21, 36, 38, 60)].\n",
      "\n",
      "#### Computational Resources\n",
      "\n",
      "Computational resources are another critical consideration. Fine-tuning involves further training pre-trained models to improve their performance on specific tasks by updating the model's parameters using a smaller, domain-specific dataset. This process can be resource-intensive but is essential for adapting models to targeted applications and domains [Data: Reports (19, 45, 31, 60)].\n",
      "\n",
      "In contrast, PEFT approaches, such as LoRA and QLoRA, are designed to enhance memory efficiency and reduce computational costs during the fine-tuning process. These techniques allow for fine-tuning on less powerful hardware while maintaining performance levels comparable to traditional methods. PEFT is particularly useful for fine-tuning multimodal models and deploying NLP models on mobile devices [Data: Reports (36, 38, 19, 35)].\n",
      "\n",
      "#### Model Customization and Contextual Relevance\n",
      "\n",
      "The desired level of model customization and contextual relevance also plays a significant role. Fine-tuning is crucial for enhancing the performance of large language models (LLMs) on specific tasks, addressing challenges such as scalability, memory requirements, and resource efficiency. It allows for the customization of models to meet specialized needs and ensures they perform optimally in their intended environments [Data: Reports (45, 31, 60)].\n",
      "\n",
      "RAG systems, on the other hand, improve the accuracy and contextuality of the outputs produced by LLMs, making them suitable for applications where context is crucial [Data: Reports (20, 21, 53)].\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Ultimately, the decision between RAG, fine-tuning, and PEFT approaches should be guided by the specific needs and constraints of the company, such as the importance of context relevance, the need for task-specific optimization, and resource availability. By carefully evaluating these factors, companies can select the most appropriate method to achieve their desired outcomes [Data: Reports (19, 20, 35, 21, 36, 38, 60, 53, 45)].\n"
     ]
    }
   ],
   "source": [
    "result = query_graphrag(\n",
    "    query=\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\",\n",
    "    method=\"global\"\n",
    ")\n",
    "print(\"Query result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d9948-ba7e-49af-ac49-62b835d62174",
   "metadata": {},
   "source": [
    "### DRIFT Search\n",
    "\n",
    "<img src=\"./media/drift_search.png\" width=1000>\n",
    "\n",
    "[Dynamic Reasoning and Inference with Flexible Traversal](https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/), or DRIFT, is a novel GraphRAG concept introduced by Microsoft as an approach to local search queries that include community information in the search process.\n",
    "\n",
    "The user's query is initially processed through [Hypothetical Document Embedding (HyDE)](https://arxiv.org/pdf/2212.10496), which creates a hypothetical document similar to those found in the graph already, but using the user's topic query. This document is embedded and used for semantic retrieval of the top-k relevant community reports. From these matches, we generate an initial answer along with several follow-up questions as a lightweight version of global search. They refer to this as the primer.\n",
    "\n",
    "Once this primer phase is complete, we execute local searches for each follow-up question generated. Each local search produces both intermediate answers and new follow-up questions, creating a refinement loop. This loop runs for two iterations (noted future research planned to develop reward functions for smarter termination). An important note that makes these local searches unique is that they are informed by both community-level knowledge and detailed entity/relationship data. This allows the DRIFT process to find relevant information even when the initial query diverges from the indexing persona, and it can adapt its approach based on emerging information during the search.\n",
    "\n",
    "The final output is structured as a hierarchy of questions and answers, ranked by their relevance to the original query. Map reduce is used again with an equal weighting on all intermediate answers, then passed to the language model for a final response. DRIFT cleverly combines global and local search with guided exploration to provide both broad context and specific details in responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0ffcac67-ab84-4dd7-bf7e-cf4b2a4e7cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result:\n",
      "INFO: Vector Store Args: {\n",
      "    \"type\": \"lancedb\",\n",
      "    \"db_uri\": \"/Users/adamlucek/Desktop/github/GraphRAG/ragtest/output/lancedb\",\n",
      "    \"container_name\": \"==== REDACTED ====\",\n",
      "    \"overwrite\": true\n",
      "}\n",
      "creating llm client with {'api_key': 'REDACTED,len=51', 'type': \"openai_chat\", 'encoding_model': 'cl100k_base', 'model': 'gpt-4o', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n",
      "creating embedding llm client with {'api_key': 'REDACTED,len=51', 'type': \"openai_embedding\", 'encoding_model': 'cl100k_base', 'model': 'text-embedding-3-small', 'max_tokens': 4000, 'temperature': 0, 'top_p': 1, 'n': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'audience': None, 'deployment_name': None, 'model_supports_json': None, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25, 'responses': None}\n",
      "\n",
      "SUCCESS: DRIFT Search Response:\n",
      "# Understanding the Choice Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When a company is faced with the decision to choose between Retrieval-Augmented Generation (RAG), fine-tuning, and various Parameter-Efficient Fine-Tuning (PEFT) approaches, several factors must be taken into account, determined by the specific application requirements and resource availability.\n",
      "\n",
      "## Retrieving Capabilities with RAG\n",
      "RAG is particularly advantageous in scenarios where it is crucial to integrate external information into language models for enhanced accuracy and relevance. It is highly recommended for applications like question and answer systems, customer support, and summarization tasks, where real-time context retrieval from large datasets improves response precision without needing extensive and expensive model retraining. The strength of RAG lies in its ability to quickly adapt to various knowledge domains by leveraging existing data without modifying the underlying language model significantly.\n",
      "\n",
      "## The Focus and Depth of Fine-Tuning\n",
      "Traditional fine-tuning is the go-to choice when a clear, defined improvement in task performance is needed using domain-specific datasets. It involves re-calibrating a pre-trained model’s parameters to cater precisely to industry-specific or task-specific requirements. This approach can enhance performance metrics significantly but incurs high computational costs and demands substantial amounts of fine-tuning data, making it ideal for well-funded projects aiming for precise task optimization.\n",
      "\n",
      "## Resource-Efficiency with PEFT Techniques\n",
      "On the other end of the spectrum, PEFT approaches such as DORA and LoRA present themselves as highly resource-efficient alternatives to standard fine-tuning. They are suitable for companies with limited computational resources but still prioritize performance enhancements. By selectively adjusting additional smaller model parameters, PEFT methods allow substantial flexibility and significant model adaptability for various applications without the full overhead of conventional fine-tuning.\n",
      "\n",
      "Ultimately, the decision for a company involves balancing resource availability (both in terms of data and computing power), desired performance efficiency, and the specific features of the task at hand. Companies with broad application needs across different domains might favor RAG's flexibility; domain-specific improvements would benefit more from deep fine-tuning, while constrained environments should consider PEFT methods for their adaptability and efficiency.\n",
      "\n",
      "# Understanding the Choice Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When choosing between Retrieval-Augmented Generation (RAG), fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) approaches, businesses must evaluate their specific needs in terms of model performance, resource constraints, and flexibility. Each approach involves distinct methods for optimally leveraging machine learning models and NLP systems.\n",
      "\n",
      "**Retrieval-Augmented Generation (RAG):** RAG is a strategy combining retrieval and generative models to enhance the context provided to language models like GPT-3. This approach allows models to answer queries more accurately by accessing external knowledge bases. It is particularly beneficial when dealing with tasks requiring extensive context or up-to-date information. Companies would favor RAG when the model needs to maintain relevance in dynamic information environments or when the contextual accuracy is crucial.\n",
      "\n",
      "**Fine-Tuning:** This traditional method involves adjusting the entire model's parameters to optimize performance on a specific task. Fine-tuning is often resource-intensive, requiring significant computational power and time, which can prove a bottleneck for large models or organizations with limited resources. It produces highly task-specific outcomes, making it an ideal choice when high accuracy is paramount and resources are abundant. Fine-tuning is suitable for specialized tasks where the nuances of data need careful embedding into the model.\n",
      "\n",
      "**Parameter-Efficient Fine-Tuning (PEFT):** Techniques like QLoRA and LoRA are designed to address the resource constraints associated with fine-tuning. By adjusting only a subset of parameters, these methods provide a more efficient approach, maintaining model performance while requiring less computational power. Companies might choose PEFT when deploying large models on resource-constrained devices, like mobile platforms, or when exploring multiple tasks with shared resources. PEFT's importance is growing in real-world applications where cost and efficiency are central concerns.\n",
      "\n",
      "In summary, choosing between these approaches depends on an organization’s priorities: RAG offers dynamic and contextual adaptability; fine-tuning promises high precision at the cost of resources; and PEFT balances resource demands with performance, making it suitable for scalable deployment. Organizations are encouraged to deeply understand their specific contextual and technical requirements to select the most appropriate approach.\n",
      "\n",
      "## How Companies Choose Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When deciding between Retrieval-Augmented Generation (RAG), fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) techniques, companies must consider a variety of factors related to their unique needs and resources. Each approach serves distinct purposes and offers specific advantages, making them suitable for different scenarios.\n",
      "\n",
      "### Retrieval-Augmented Generation (RAG)\n",
      "RAG is particularly beneficial when a company requires a model to access a vast external data set to create highly contextual and informed responses. This method is preferred when the task involves processing or retrieving large amounts of dynamic information. The power of RAG lies in its ability to provide more comprehensive answers by leveraging external data sources, enhancing the model's output quality. As such, organizations where information retrieval and integration are crucial components of the application, such as in customer support systems or knowledge-based industries, may find RAG an optimal choice.\n",
      "\n",
      "### Fine-Tuning\n",
      "Fine-tuning a large language model involves adjusting its parameters based on specific tasks or domain data to improve its performance on said tasks. This method is ideal for companies aiming to develop a bespoke solution tailored to precise needs or niche applications. Fine-tuning a model can significantly increase its utility in specialized fields such as legal research or medical analysis, where the accuracy and specificity of information are critical. Though resource-intensive, the specificity in outcome makes traditional fine-tuning valuable for businesses that require high degrees of customization.\n",
      "\n",
      "### Parameter-Efficient Fine-Tuning (PEFT)\n",
      "PEFT techniques, such as LoRA and adapter-based methods, offer a way to enhance large language models without the typical computational cost associated with traditional fine-tuning. This strategy updates only a small subset of a model's parameters, thus making it a cost-effective alternative for companies with limited resources or those that need to frequently update models with new data. PEFT is advantageous for businesses that need flexibility without a significant investment in computational power, popular among startups or organizations exploring new avenues without extensive AI infrastructure.\n",
      "\n",
      "In summary, a company's choice between RAG, fine-tuning, and PEFT depends on their specific requirements, the nature and volume of data they interact with, and the computational resources at their disposal. By aligning their strategy with the strengths of each approach, businesses can optimize their use of AI technologies to meet their unique operational goals.\n",
      "\n",
      "# Evaluating Company Choices Among RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When a company considers enhancing its machine learning models, particularly large language models (LLMs), it encounters a range of techniques, including Retrieval-Augmented Generation (RAG), fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) methods like Half Fine-Tuning (HFT). Each of these techniques offers unique benefits and is suitable for different objectives and scenarios. Understanding the distinction and potential applications is crucial for making an informed decision.\n",
      "\n",
      "RAG is highly effective in situations where models need to provide accurate responses based on external information not inherently within the model's parameters. By integrating information retrieval techniques, RAG allows models to access a database or document store to augment their generated text, making it ideal for applications where dynamic content access and up-to-date information are critical, such as live customer service or research queries.\n",
      "\n",
      "In contrast, fine-tuning broadly involves adjusting an already pre-trained model to suit new, specific tasks or datasets. Traditional supervised fine-tuning is useful when the primary goal is to improve a model’s performance on specific tasks while leveraging existing knowledge. For example, in supervised fine-tuning, Half Fine-Tuning (HFT) offers an innovative balance by freezing a portion of the model's parameters, thus maintaining foundational knowledge while developing new competencies. This efficiency is especially important when handling models like the LLAMA 2-7B, where sustaining knowledge over several iterations is vital.\n",
      "\n",
      "PEFT techniques, such as HFT, provide a focused approach to fine-tuning by optimizing resource usage and reducing computational demands. They ensure that changes to the model preserve core functionalities while adapting to new requirements, which is critically important in environments where computational resources and model interpretability are constrained.\n",
      "\n",
      "The choice among RAG, fine-tuning, and PEFT approaches depends on factors such as the company's resource availability, existing model performance, specific use-cases, and the need for adaptability versus resource conservation. Companies prioritizing task-specific improvements with limited computational power might lean toward PEFT or HFT, whereas RAG would be preferable for scenarios needing comprehensive and current information access capabilities.\n",
      "\n",
      "Ultimately, the decision must align with strategic objectives, model lifecycle considerations, and the specific complexities of the data and questions the model is expected to handle.\n",
      "\n",
      "# Factors in Choosing Between RAG, Fine-Tuning, and PEFT Approaches\n",
      "\n",
      "When a company is deciding between Retrieval-Augmented Generation (RAG), fine-tuning, and Parameter-Efficient Fine-Tuning (PEFT) methods for large language models (LLMs), several important factors must be considered. These methods each have unique advantages and challenges that make them suitable for different scenarios.\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) enhances the performance of LLMs by integrating retrieval mechanisms that bring relevant information into the context before generating results. This approach is particularly useful when the task requires the contextually relevant retrieval of information from extensive databases or when the generation task is highly dependent on external knowledge. RAG systems improve both the accuracy and the contextual relevance of LLM outputs, making them valuable for applications where precision in response to contextual queries is critical.\n",
      "\n",
      "Fine-tuning involves adjusting the LLM using a carefully prepared dataset tailored to a specific task. This process includes essential stages such as dataset preparation, instruction tuning, and topic mapping. Fine-tuning is especially advantageous when the intention is to specialize a model for a specific application, ensuring nuanced comprehension and performance improvement for the target task. It provides more control over the model's behavior, as specific datasets align the model with intended outcomes, offering enhanced reliability.\n",
      "\n",
      "Parameter-Efficient Fine-Tuning (PEFT) approaches are designed to fine-tune LLMs with fewer resource requirements compared to traditional fine-tuning. Methods like LoRA (Low-Rank Adaptation), Adapter modules, and prefix tuning are examples of PEFT techniques. They focus on adapting a small set of parameters while maintaining the majority of the LLM fixed, thus reducing computational costs and the necessary dataset size. PEFT is particularly beneficial for scenarios with constrained computational resources or when rapidly deploying updates or adaptations is necessary without retraining the entire model.\n",
      "\n",
      "Ultimately, the choice between these approaches depends on the specific requirements and constraints of the task at hand, including the need for real-time retrieval capabilities (favoring RAG), the specificity and quality of output (favoring fine-tuning), or capacity and resource efficiency (favoring PEFT).\n"
     ]
    }
   ],
   "source": [
    "result = query_graphrag(\n",
    "    query=\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\",\n",
    "    method=\"drift\"\n",
    ")\n",
    "print(\"Query result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e04a5-aa55-4955-b2c0-b86dd462f549",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing to Regular Vector Database Retrieval\n",
    "\n",
    "<img src=\"./media/basic_retrieval.png\" width=600>\n",
    " \n",
    "To give some comparison, let's look back at traditional chunking, embedding, and similarity retrieval RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e12fa20-81d9-43cc-a641-6e8ba7cb5e9b",
   "metadata": {},
   "source": [
    "**Instantiate our Database**\n",
    "\n",
    "For this we'll be using [ChromaDB](https://www.trychroma.com) with the same chunks as were loaded into our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361a631-4e98-47d5-9b2d-48810c2eab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./notebook/chromadb\")\n",
    "paper_collection = chroma_client.get_or_create_collection(name=\"paper_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a160165-6ed9-4ac9-be3b-db5bff88beb4",
   "metadata": {},
   "source": [
    "**Embed Chunks Into Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24350f0f-32ef-41ea-9d7c-5e970ca172f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for text in texts:\n",
    "    paper_collection.add(\n",
    "        documents=[text],\n",
    "        ids=f\"chunk_{i}\"\n",
    "    )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3961f0-bdce-4d1d-8d49-86355bb7d505",
   "metadata": {},
   "source": [
    "**Retrieval Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f2276-fc27-4008-9e63-6ec9d4db66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_retrieval(query, num_results=5):\n",
    "    results = paper_collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=num_results\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2bb61e-5d36-47fc-b2ec-6557293f5ef4",
   "metadata": {},
   "source": [
    "**RAG Prompt & Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "593a23ff-c34c-47b4-b6d8-c2ca59fad69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template = \"\"\"\n",
    "Generate a response of the target length and format that responds to the user's question, summarizing all information in the input data tables appropriate for the response length and format, and incorporating any relevant general knowledge.\n",
    "\n",
    "If you don't know the answer, just say so. Do not make anything up.\n",
    "\n",
    "Do not include information where the supporting evidence for it is not provided.\n",
    "\n",
    "Context: {retrieved_docs}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)\n",
    "\n",
    "rag_chain = rag_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211bbff-9927-464b-942d-964005707475",
   "metadata": {},
   "source": [
    "**RAG Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e654479e-0d40-4de3-8d03-1de08b54a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chroma_rag(query):\n",
    "    retrieved_docs = chroma_retrieval(query)[\"documents\"][0]\n",
    "    response = rag_chain.invoke({\"retrieved_docs\": retrieved_docs, \"query\": query})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba855e-b29c-4169-8302-b1b5cceee76c",
   "metadata": {},
   "source": [
    "**RAG Response**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "72201a27-6dac-4f3b-8ff9-717f4998470c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
       "</pre>\n"
      ],
      "text/plain": [
       "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When choosing between Retrieval-Augmented Generation (RAG), fine-tuning, and different Parameter-Efficient Fine-Tuning (PEFT) approaches, a company should consider several factors:\n",
      "\n",
      "1. **Data Access and Updates**: RAG is preferable for applications requiring access to external data sources or environments where data frequently updates. It provides dynamic data retrieval capabilities and is less prone to generating incorrect information.\n",
      "\n",
      "2. **Model Behavior and Domain-Specific Knowledge**: Fine-tuning is suitable when the model needs to adjust its behavior, writing style, or incorporate domain-specific knowledge. It is effective if there is ample domain-specific, labeled training data available.\n",
      "\n",
      "3. **Resource Constraints and Efficiency**: PEFT approaches like LoRA and DEFT are designed to reduce computational and resource requirements. LoRA focuses on low-rank matrices to reduce memory usage and computational load, while DEFT optimizes the fine-tuning process by focusing on the most critical data samples.\n",
      "\n",
      "4. **Task-Specific Adaptation**: If the goal is to adapt a model for specific tasks with minimal data, PEFT methods like DEFT and adapter-based techniques can be beneficial. They allow for efficient fine-tuning with fewer resources.\n",
      "\n",
      "5. **Transparency and Interpretability**: RAG systems offer more transparency and interpretability in the model’s decision-making process compared to solely fine-tuned models.\n",
      "\n",
      "6. **Scalability and Deployment**: For large-scale deployments, PEFT methods can significantly reduce computational costs by focusing on influential data samples and using surrogate models.\n",
      "\n",
      "In summary, the choice depends on the specific needs of the application, such as data access, model behavior, resource availability, and the importance of transparency and scalability.\n"
     ]
    }
   ],
   "source": [
    "response = chroma_rag(\"How does a company choose between RAG, fine-tuning, and different PEFT approaches?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a630f-3268-45ec-b58f-dbb42106864a",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion\n",
    "\n",
    "**Traditional/Naive RAG:**\n",
    "\n",
    "Benefits:\n",
    "- Simpler implementation and deployment\n",
    "- Works well for straightforward information retrieval tasks\n",
    "- Good at handling unstructured text data\n",
    "- Lower computational overhead\n",
    "\n",
    "Drawbacks:\n",
    "- Loses structural information when chunking documents\n",
    "- Can break up related content during text segmentation\n",
    "- Limited ability to capture relationships between different pieces of information\n",
    "- May struggle with complex reasoning tasks requiring connecting multiple facts\n",
    "- Potential for incomplete or fragmented answers due to chunking boundaries\n",
    "\n",
    "**GraphRAG:**\n",
    "\n",
    "Benefits:\n",
    "- Preserves structural relationships and hierarchies in the knowledge\n",
    "- Better at capturing connections between related information\n",
    "- Can provide more complete and contextual answers\n",
    "- Improved retrieval accuracy by leveraging graph structure\n",
    "- Better supports complex reasoning across multiple facts\n",
    "- Can maintain document coherence better than chunk-based approaches\n",
    "- More interpretable due to explicit knowledge representation\n",
    "\n",
    "Drawbacks:\n",
    "- More complex to implement and maintain\n",
    "- Requires additional processing to construct and update knowledge graphs\n",
    "- Higher computational overhead for graph operations\n",
    "- May require domain expertise to define graph schema/structure\n",
    "- More challenging to scale to very large datasets\n",
    "- Additional storage requirements for graph structure\n",
    "\n",
    "**Key Differentiators:**\n",
    "1. Knowledge Representation: Traditional RAG treats everything as flat text chunks, while GraphRAG maintains structured relationships in a graph format\n",
    "\n",
    "2. Context Preservation: GraphRAG better preserves context and relationships between different pieces of information compared to the chunking approach of traditional RAG\n",
    "\n",
    "3. Reasoning Capability: GraphRAG enables better multi-hop reasoning and connection of related facts through graph traversal, while traditional RAG is more limited to direct retrieval\n",
    "\n",
    "4. Answer Quality: GraphRAG tends to produce more complete and coherent answers since it can access related information through graph connections rather than being limited by chunk boundaries\n",
    "\n",
    "The choice between traditional RAG and GraphRAG often depends on the specific use case, with GraphRAG being particularly valuable when maintaining relationships between information is important or when complex reasoning is required. An important note as well, GraphRAG approaches still rely on regular embedding and retrieval methods themselves. They compliment eahcother!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c5872-2185-46dc-aaef-2108bc490a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
