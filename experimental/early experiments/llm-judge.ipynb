{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src import get_azure_openai_model, get_azure_openai_chat_model, create_vector_store\n",
    "\n",
    "model = get_azure_openai_chat_model()\n",
    "vector_store = create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDFs\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "folder_path = \"papers\"\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "pages = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(os.path.join(folder_path, pdf_file))\n",
    "    async for page in loader.alazy_load():\n",
    "        pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add pdf pages to vector store\n",
    "_ = vector_store.add_documents(documents=pages)\n",
    "\n",
    "# Retrival\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic setup for langsmith experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import wrappers, Client\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "client = Client()\n",
    "openai_client = wrappers.wrap_openai(OpenAI(api_key=subscription_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset [refrence from langsmith page]\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file_path = os.path.join(folder_path, 'hpb7answers.csv')\n",
    "df = pd.read_csv(csv_file_path, encoding='latin1')\n",
    "\n",
    "# Ensure inputs and outputs are in the correct format\n",
    "examples = [(input_text, output_text) for input_text, output_text in zip(df['Question'].tolist(), df['Answer'].tolist())]\n",
    "\n",
    "inputs = [{\"question\": input_prompt} for input_prompt, _ in examples]\n",
    "outputs = [{\"answer\": output_answer} for _, output_answer in examples]\n",
    "\n",
    "\n",
    "# Programmatically create a dataset in LangSmith\n",
    "dataset = client.create_dataset(\n",
    "\tdataset_name = \"Sample dataset2\",\n",
    "\tdescription = \"A sample dataset in LangSmith.\"\n",
    ")\n",
    "\n",
    "# Add examples to the dataset\n",
    "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset [refrence from langsmith page]\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file_path = os.path.join(folder_path, 'hpb7answers.csv')\n",
    "df = pd.read_csv(csv_file_path, encoding='latin1')\n",
    "\n",
    "# Ensure inputs and outputs are in the correct format\n",
    "examples = [{'input': input_text, 'output': output_text} for input_text, output_text in zip(df['Question'].tolist(), df['Answer'].tolist())]\n",
    "\n",
    "inputs = [examples[i][\"input\"] for i in range(len(examples))]\n",
    "outputs = [examples[i][\"output\"] for i in range(len(examples))]\n",
    "\n",
    "\n",
    "# Programmatically create a dataset in LangSmith\n",
    "dataset = client.create_dataset(\n",
    "\tdataset_name = \"Sample dataset\",\n",
    "\tdescription = \"A sample dataset in LangSmith.\"\n",
    ")\n",
    "\n",
    "# Add examples to the dataset\n",
    "client.create_examples(inputs=examples, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the application logic for evaluation\n",
    "\n",
    "# TO-DO: Evaluate the self-eval model\n",
    "\n",
    " \n",
    "\n",
    "def target(inputs: dict) -> dict:\n",
    "    question = inputs[\"input\"]\n",
    "    # Prompt\n",
    "    system = \"\"\"Answer the following question accurately\"\"\"\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    retrieval_grader = grade_prompt | model | StrOutputParser()\n",
    "   \n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return {\"response\": retrieval_grader.invoke({\"question\": question, \"document\": docs})}\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'Harry accuses Remus Lupin of being a coward in \"Harry Potter and the Deathly Hallows.\" This accusation arises during a heated conversation about Lupin\\'s decision to leave Tonks and their unborn child to join Harry and the others in the fight against Voldemort. Harry feels that Lupin is abandoning his family out of fear for their safety, which he interprets as cowardice. Harry\\'s frustration stems from his own experiences with loss and sacrifice, and he believes that fighting against evil is a responsibility that should not be avoided, even at great personal cost. This moment highlights the themes of bravery, sacrifice, and the complexities of personal choices in the face of danger.'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instructions for the LLM judge evaluator\n",
    "instructions = \"\"\"Evaluate Answer against Ground Truth for conceptual similarity and classify true or false: \n",
    "- False: No conceptual match and similarity\n",
    "- True: Most or full conceptual match and similarity\n",
    "- Key criteria: Concept should match, not exact wording.\n",
    "\"\"\"\n",
    "\n",
    "# Define output schema for the LLM judge\n",
    "class Grade(BaseModel):\n",
    "    score: bool = Field(description=\"Boolean that indicates whether the response is accurate relative to the reference answer\")\n",
    "\n",
    "# Define LLM judge that grades the accuracy of the response relative to reference output\n",
    "def accuracy(outputs: dict, reference_outputs: dict) -> bool:\n",
    "  \n",
    "  response = openai_client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "      { \"role\": \"system\", \"content\": instructions },\n",
    "      { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]}; \n",
    "      Student's Answer: {outputs[\"response\"]}\"\"\"\n",
    "  }],\n",
    "    response_format=Grade\n",
    "  );\n",
    "  return response.choices[0].message.parsed.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'first-eval-in-langsmith-532bfb7b' at:\n",
      "https://smith.langchain.com/o/6fe80679-05e1-4b72-908a-9af5dfa86dd5/datasets/83fb0beb-9b1c-435c-ae28-cbb9cec1f593/compare?selectedSessions=2426e413-a5c9-4cc9-ae69-ef69d630e2da\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator accuracy> on run c425ef2d-050b-4a3e-a595-81a00f7baf25: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "1it [00:01,  1.46s/it]Error running evaluator <DynamicRunEvaluator accuracy> on run 9051c136-43d2-4f19-ba37-f3a23c68bb2c: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "2it [00:01,  1.47it/s]Error running evaluator <DynamicRunEvaluator accuracy> on run c622d464-ab29-4b05-8d15-e66cd17ae37b: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "3it [00:02,  1.43it/s]Error running evaluator <DynamicRunEvaluator accuracy> on run 112e3d0b-b7f0-490d-930d-4763be872409: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator accuracy> on run 3f70ca58-327a-4905-9bda-71bb2e456e89: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "5it [00:03,  1.88it/s]Error running evaluator <DynamicRunEvaluator accuracy> on run 4427035e-bc96-4038-8ecc-8f870cd80351: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "6it [00:04,  1.41it/s]Error running evaluator <DynamicRunEvaluator accuracy> on run f5b740c0-0864-4cc6-ac99-e3a20ef304ea: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator accuracy> on run 30e46b12-2504-480e-8b31-5185fc8e80c7: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "8it [00:05,  1.57it/s]Error running evaluator <DynamicRunEvaluator accuracy> on run ef5ef345-00ec-41c0-b475-19f843691ff9: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "9it [00:05,  1.67it/s]Error running evaluator <DynamicRunEvaluator accuracy> on run 7dfc51df-9e72-4c27-8f70-ef08567ec09e: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "10it [00:06,  1.89it/s]Error running evaluator <DynamicRunEvaluator accuracy> on run fe8800cc-a8dd-4a4d-bbe1-346073e71679: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "11it [00:07,  1.39it/s]Error running evaluator <DynamicRunEvaluator accuracy> on run 21fb568d-fdb5-460e-b8e5-857d6a91c75f: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator accuracy> on run 403073a6-65d0-42ab-acae-3e5af0d7450b: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "13it [00:08,  1.39it/s]Error running evaluator <DynamicRunEvaluator accuracy> on run 1baba773-295b-4866-ab0e-7f1526de4805: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "14it [00:12,  1.45s/it]Error running evaluator <DynamicRunEvaluator accuracy> on run 1d5d3679-0486-4ae1-b4bb-571c707a3421: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "Error running evaluator <DynamicRunEvaluator accuracy> on run a6bedfa8-d9ee-4ac5-87ca-625d0ddccb70: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "16it [01:02, 10.94s/it]Error running evaluator <DynamicRunEvaluator accuracy> on run 62d60bb6-43ef-4d8b-9758-e9d4dd4d14ad: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "17it [01:03,  8.80s/it]Error running evaluator <DynamicRunEvaluator accuracy> on run d2d21d37-f305-471c-8be5-7c98973b7612: KeyError('answer')\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1573, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 331, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 617, in wrapper\n",
      "    raise e\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 614, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 694, in wrapper\n",
      "    return func(*args)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\newac\\AppData\\Local\\Temp\\ipykernel_5640\\3429938280.py\", line 18, in accuracy\n",
      "    { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]};\n",
      "                                                          ~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "KeyError: 'answer'\n",
      "18it [01:04,  3.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data = \"Sample dataset\",\n",
    "    evaluators = [\n",
    "        accuracy,\n",
    "        # can add multiple evaluators here\n",
    "    ],\n",
    "    experiment_prefix = \"first-eval-in-langsmith\",\n",
    "    max_concurrency = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
