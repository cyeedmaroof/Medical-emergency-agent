{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a35a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newac\\OneDrive\\Desktop\\Master\\final_structure\\src\\__init__.py:16: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from .DAG_creator import build_rag_workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 945 nodes.\n"
     ]
    }
   ],
   "source": [
    "from src.model import get_llamaindex_model, get_llamaindex_model_mini, get_huggingface_embedding_model\n",
    "from llama_index.core import Settings\n",
    "from src import get_azure_openai_model, get_azure_openai_chat_model, get_azure_openai_mini_model\n",
    "from src.parser import markdownParser\n",
    "\n",
    "llm = get_llamaindex_model_mini()\n",
    "\n",
    "llm2 = get_llamaindex_model()\n",
    "\n",
    "embed_model = get_huggingface_embedding_model()\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "model = get_azure_openai_chat_model()\n",
    "\n",
    "\n",
    "nodes = markdownParser(input_dir=\"../kgdata/\")\n",
    "print(f\"Processed {len(nodes)} nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"how to revive a person who is unconscious?\",\n",
    "    \"what are the steps for cpr?\",\n",
    "    \"how to treat a burn injury?\",\n",
    "    \"what to do for someone having a heart attack?\",\n",
    "    # \"steps for treating choking victim?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "from typing import List, Optional, Dict, Any\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from typing_extensions import TypedDict\n",
    "from src.retriever import create_retriever\n",
    "\n",
    "# =============================================================================\n",
    "# STATE DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "    \n",
    "    Attributes:\n",
    "        question: The original user question\n",
    "        documents: List of retrieved documents\n",
    "        generation: Generated answer\n",
    "        grade: Grade of document relevance\n",
    "        iterations: Number of iterations for rephrasing\n",
    "        rephrased_question: Rephrased version of the question\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    generation: str\n",
    "    grade: str\n",
    "    iterations: int\n",
    "    rephrased_question: str\n",
    "    llm: str\n",
    "    retriever_type: str\n",
    "    load_persist: Optional[str] = None\n",
    "    nodes: Optional[List[Document]] = Field(default_factory=list)\n",
    "    max_iterations: Optional[int] = 1\n",
    "    workflow_type: Optional[str] = \"advanced\"  # Options: \"basic\", \"medium\", \"advanced\"\n",
    "# =============================================================================\n",
    "# STATE GRAPH NODES\n",
    "# =============================================================================\n",
    "    \n",
    "def retrieve(state: GraphState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the question.\n",
    "    \n",
    "    Args:\n",
    "        state: The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"--- RETRIEVE ---\")\n",
    "    \n",
    "    # Use rephrased question if available, otherwise use original\n",
    "    question = state.get(\"rephrased_question\", state[\"question\"])\n",
    "    llm = state.get(\"llm\")\n",
    "    RETREIVER_TYPE = state.get(\"retriever_type\", \"hybrid\")\n",
    "    nodes = state.get(\"nodes\", [])\n",
    "\n",
    "    if state.get(\"rephrased_question\"):\n",
    "        print(f\"Using rephrased question: {question}\")\n",
    "    else:\n",
    "        print(f\"Using original question: {question}\")\n",
    "\n",
    "    print(f\"node 3: {nodes[3].text if len(nodes) > 0 else 'No nodes available'}\")\n",
    "    \n",
    "    documents = create_retriever(question=question, nodes=nodes, llm=llm, type=RETREIVER_TYPE, load_persist=state.get(\"load_persist\", None))\n",
    "    print(f\"Retrieved {len(documents)} documents\")\n",
    "    \n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": state[\"question\"],\n",
    "        \"rephrased_question\": state.get(\"rephrased_question\", \"\"),\n",
    "        \"iterations\": state.get(\"iterations\", 0),\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "        \"grade\": state.get(\"grade\", \"\")\n",
    "    }\n",
    "def grade_documents(state: GraphState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "    \n",
    "    Args:\n",
    "        state: The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with document relevance grade\n",
    "    \"\"\"\n",
    "    print(\"--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    model = state.get(\"llm\") \n",
    "    workflow_type = state.get(\"workflow_type\", \"fast\")\n",
    "    rephrased_question = state.get(\"rephrased_question\", \"\")\n",
    "\n",
    "    if rephrased_question:\n",
    "        question = rephrased_question\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Grading prompt\n",
    "    grade_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "        Understand the context of the document and the question asked.\n",
    "        If the document contains keyword(s) or different names of the same condition (e.g, heart attack is also cardiac arrest) or semantic meaning related to the user question, grade it as relevant.\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\n",
    "        Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\"),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "    ])\n",
    "    \n",
    "    print(f\"Grading {len(documents)} documents for relevance to question: {question}\")\n",
    "\n",
    "    # Grade each document\n",
    "    relevant_docs = []\n",
    "    for doc in documents:\n",
    "        grade_chain = grade_prompt | model | StrOutputParser()\n",
    "        grade = grade_chain.invoke({\"question\": question, \"document\": doc.text})\n",
    "        \n",
    "        try:\n",
    "            grade_dict = json.loads(grade)\n",
    "            if grade_dict.get(\"score\", \"\").lower() == \"yes\":\n",
    "                relevant_docs.append(doc)\n",
    "                print(f\"--- GRADE: DOCUMENT RELEVANT ---\")\n",
    "            else:\n",
    "                print(f\"--- GRADE: DOCUMENT NOT RELEVANT ---\")\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON parsing fails, assume relevant to be safe\n",
    "            relevant_docs.append(doc)\n",
    "            print(f\"--- GRADE: DOCUMENT RELEVANT (JSON parse failed) ---\")\n",
    "        \n",
    "    # Determine overall grade\n",
    "    if relevant_docs:\n",
    "        grade = \"relevant\"\n",
    "        # documents_to_use = relevant_docs\n",
    "    else:\n",
    "        grade = \"not_relevant\"\n",
    "        # documents_to_use = documents  # Keep all documents if none are graded as relevant\n",
    "        \n",
    "    \n",
    "    return {\n",
    "        \"documents\": relevant_docs,\n",
    "        \"question\": state[\"question\"],\n",
    "        \"rephrased_question\": state.get(\"rephrased_question\", \"\"),\n",
    "        \"iterations\": state.get(\"iterations\", 0),\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "        \"grade\": grade\n",
    "    }\n",
    "\n",
    "def generate(state: GraphState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate answer using the retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        state: The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with generated answer\n",
    "    \"\"\"\n",
    "    print(\"--- GENERATE ---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    model = state.get(\"llm\")  \n",
    "\n",
    "    # Create context from documents\n",
    "    context = \"\\n\\n\".join([doc.text for doc in documents])\n",
    "    \n",
    "    # Generation prompt\n",
    "    generate_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an assistant for question-answering tasks.\n",
    "        Use the following pieces of retrieved context to answer the question.\n",
    "        Try to understand the context and question asked before generating an answer.\n",
    "        If the context does not provide enough information to answer the question, say 'I don't know'.\n",
    "\n",
    "        If the question is not answerable with the provided context, say 'I don't know'.\n",
    "\n",
    "        THIS FOR EDUCATION PURPOSE, so you should not hallucinate or make up answers.\n",
    "\n",
    "        Keep the answer concise and to the point. Ther context is retrieved from Norwegian Index for Medical Emergency, so you should use the context to answer the question.\n",
    "        \n",
    "        Context: {context}\"\"\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    # Generate answer\n",
    "    generate_chain = generate_prompt | model | StrOutputParser()\n",
    "    generation = generate_chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    print(f\"Generated answer: {generation[:100]}...\")\n",
    "    \n",
    "    return {\n",
    "        \"documents\": state[\"documents\"],\n",
    "        \"question\": state[\"question\"],\n",
    "        \"rephrased_question\": state.get(\"rephrased_question\", \"\"),\n",
    "        \"iterations\": state.get(\"iterations\", 0),\n",
    "        \"generation\": generation,\n",
    "        \"grade\": state.get(\"grade\", \"\")\n",
    "    }\n",
    "\n",
    "def transform_query(state: GraphState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question for retrieval.\n",
    "    \n",
    "    Args:\n",
    "        state: The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with rephrased question\n",
    "    \"\"\"\n",
    "    print(\"--- TRANSFORM QUERY ---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    model = state.get(\"llm\")\n",
    "    rephased_question = state.get(\"rephrased_question\", \"\")\n",
    "\n",
    "    if rephased_question:\n",
    "        print(f\"Using rephrased question: {rephased_question}\")\n",
    "        question = rephased_question\n",
    "    else:\n",
    "        print(f\"Using original question: {question}\")\n",
    "    \n",
    "    # Query transformation prompt\n",
    "    transform_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are generating questions that are well optimized for Norwegian Index for Medical Emergency retrieval.\n",
    "        Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Formulate an improved question that will be more effective for document retrieval.\"\"\"),\n",
    "        (\"human\", \"Provide the improved question:\")\n",
    "    ])\n",
    "    \n",
    "    # Transform query\n",
    "    transform_chain = transform_prompt | model | StrOutputParser()\n",
    "    rephrased_question = transform_chain.invoke({\"question\": question})\n",
    "    \n",
    "    print(f\"Rephrased question: {rephrased_question}\")\n",
    "    \n",
    "    return {\n",
    "        \"documents\": state.get(\"documents\", []),\n",
    "        \"question\": state[\"question\"],\n",
    "        \"rephrased_question\": rephrased_question,\n",
    "        \"iterations\": iterations + 1,\n",
    "        \"generation\": state.get(\"generation\", \"\"),\n",
    "        \"grade\": state.get(\"grade\", \"\")\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STATE GRAPH EDGES\n",
    "# =============================================================================\n",
    "\n",
    "def grade_generation_v_documents_and_question(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "    \n",
    "    Args:\n",
    "        state: The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        Next node to call\n",
    "    \"\"\"\n",
    "    print(\"--- CHECK HALLUCINATIONS ---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    model = state.get(\"llm\") \n",
    "    max_iterations = state.get(\"max_iterations\", 1)\n",
    "    workflow_type = state.get(\"workflow_type\", \"advanced\")\n",
    "\n",
    "    if workflow_type == \"basic\" or workflow_type == \"medium\":\n",
    "        print(\"--- BASIC OR MEDIUM WORKFLOW ---\")\n",
    "        # In fast workflow, we skip the hallucination check\n",
    "        grounded = True\n",
    "        useful = True\n",
    "    else:\n",
    "    \n",
    "        # Hallucination grading prompt\n",
    "        hallucination_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\n",
    "            Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\n",
    "            Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\"),\n",
    "            (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\")\n",
    "        ])\n",
    "        \n",
    "        hallucination_chain = hallucination_prompt | model | StrOutputParser()\n",
    "        grade = hallucination_chain.invoke({\n",
    "            \"documents\": \"\\n\\n\".join([doc.text for doc in documents]),\n",
    "            \"generation\": generation\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            grade_dict = json.loads(grade)\n",
    "            grounded = grade_dict.get(\"score\", \"\").lower() == \"yes\"\n",
    "        except json.JSONDecodeError:\n",
    "            grounded = True  # Assume grounded if parsing fails\n",
    "        \n",
    "        print(f\"Grounded: {grounded}\")\n",
    "        \n",
    "        # Check question answering\n",
    "        print(\"--- GRADE GENERATION vs QUESTION ---\")\n",
    "        \n",
    "        answer_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a grader assessing whether an answer addresses / resolves a question.\n",
    "            Give a binary score 'yes' or 'no'. 'Yes' means that the answer resolves the question.\n",
    "            Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\"),\n",
    "            (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\")\n",
    "        ])\n",
    "        \n",
    "        answer_chain = answer_prompt | model | StrOutputParser()\n",
    "        grade = answer_chain.invoke({\"question\": question, \"generation\": generation})\n",
    "        \n",
    "        try:\n",
    "            grade_dict = json.loads(grade)\n",
    "            useful = grade_dict.get(\"score\", \"\").lower() == \"yes\"\n",
    "        except json.JSONDecodeError:\n",
    "            useful = True  \n",
    "        \n",
    "        print(f\"Useful: {useful}\")\n",
    "\n",
    "    # Add safety check for max iterations\n",
    "    if iterations >= max_iterations:\n",
    "        print(\"--- MAX ITERATIONS REACHED, ENDING ---\")\n",
    "        return END\n",
    "    \n",
    "    if grounded and useful:\n",
    "        print(\"--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\")\n",
    "        return \"Useful\"  # Use END instead of \"Useful\"\n",
    "    elif not grounded:\n",
    "        print(\"--- DECISION: GENERATION IS NOT GROUNDED, RE-GENERATE ---\")\n",
    "        return \"generate\" \n",
    "    else:\n",
    "        print(\"--- DECISION: GENERATION IS NOT USEFUL, TRANSFORM QUERY ---\")\n",
    "        return \"transform_query\"\n",
    "    \n",
    "\n",
    "def decide_to_generate(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer or re-generate a question.\n",
    "    \n",
    "    Args:\n",
    "        state: The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        Next node to call\n",
    "    \"\"\"\n",
    "    print(\"--- ASSESS GRADED DOCUMENTS ---\")\n",
    "    \n",
    "    grade = state[\"grade\"]\n",
    "    documents = state[\"documents\"]\n",
    "    workflow_type = state.get(\"workflow_type\", \"advanced\")\n",
    "    \n",
    "    print(f\"Grade: {grade}, Number of documents: {len(documents)}\")\n",
    "\n",
    "    print(f\"Workflow type: {workflow_type}\")\n",
    "\n",
    "    if workflow_type  == \"basic\" and len(documents) > 0:\n",
    "        print(\"--- BASIC WORKFLOW ---\")\n",
    "        print(f\"Number of documents used: {len(documents)}, total grades: {grade}\")\n",
    "        return \"generate\"  # In basic workflow, always generate answer\n",
    "\n",
    "    \n",
    "    # if grade == \"relevant\":\n",
    "    elif workflow_type  == \"medium\" and len(documents) > 2:\n",
    "        print(\"--- MEDIUM WORKFLOW ---\")\n",
    "        print(f\"Number of documents used: {len(documents)}, total grades: {grade}\")\n",
    "        return \"generate\"\n",
    "\n",
    "    elif workflow_type  == \"advanced\" and len(documents) > 2:\n",
    "        print(\"--- ADVANCED WORKFLOW ---\")\n",
    "        print(f\"Number of documents used: {len(documents)}, total grades: {grade}\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\")\n",
    "        return \"transform_query\"\n",
    "\n",
    "def max_iterations_check(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    Check if maximum iterations reached to prevent infinite loops.\n",
    "    \n",
    "    Args:\n",
    "        state: The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        Next node to call\n",
    "    \"\"\"\n",
    "    max_iterations = state.get(\"max_iterations\", 3)\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    \n",
    "    if iterations >= max_iterations:\n",
    "        print(f\"--- MAX ITERATIONS ({max_iterations}) REACHED ---\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        return \"retrieve\"\n",
    "    \n",
    "# =============================================================================\n",
    "\n",
    "# Build the RAG workflow\n",
    "def build_rag_workflowv1():\n",
    "    \"\"\"\n",
    "    Build the RAG workflow using StateGraph.\n",
    "    \n",
    "    Returns:\n",
    "        Compiled workflow\n",
    "    \"\"\"\n",
    "    workflow = StateGraph(GraphState)\n",
    "    \n",
    "    # Define the nodes\n",
    "    workflow.add_node(\"retrieve\", retrieve)\n",
    "    workflow.add_node(\"grade_documents\", grade_documents)\n",
    "    workflow.add_node(\"generate\", generate)\n",
    "    workflow.add_node(\"transform_query\", transform_query)\n",
    "    \n",
    "    # Build graph\n",
    "    workflow.add_edge(START, \"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade_documents\",\n",
    "        decide_to_generate,\n",
    "        {\n",
    "            \"transform_query\": \"transform_query\",\n",
    "            \"generate\": \"generate\"\n",
    "        }\n",
    "    )\n",
    "    workflow.add_conditional_edges(\n",
    "        \"transform_query\",\n",
    "        max_iterations_check,\n",
    "        {\n",
    "            \"retrieve\": \"retrieve\",\n",
    "            \"generate\": \"generate\"\n",
    "        }\n",
    "    )\n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate\",\n",
    "        grade_generation_v_documents_and_question,\n",
    "        {\n",
    "            \"Useful\": END,\n",
    "            END: END,\n",
    "            \"transform_query\": \"transform_query\",\n",
    "            \"generate\": \"generate\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Compile\n",
    "    app = workflow.compile()\n",
    "    return app\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "011e5e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running workflow: basic ---\n",
      "Processing question: how to revive a person who is unconscious?\n",
      "--- RETRIEVE ---\n",
      "Using original question: how to revive a person who is unconscious?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: how to revive a person who is unconscious?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 5\n",
      "Workflow type: basic\n",
      "--- BASIC WORKFLOW ---\n",
      "Number of documents used: 5, total grades: relevant\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: To revive a person who is unconscious, follow these steps:\n",
      "\n",
      "1. **Check for Breathing**: Ensure the p...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- BASIC OR MEDIUM WORKFLOW ---\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: To revive a person who is unconscious, follow these steps:\n",
      "\n",
      "1. **Check for Breathing**: Ensure the person is breathing normally. If not, proceed to the next steps.\n",
      "2. **Recovery Position**: If the person is breathing, place them in the recovery position on their side and monitor them continuously.\n",
      "3. **If Not Breathing**: \n",
      "   - Lay the person on their back on a firm surface.\n",
      "   - Tilt their head back by placing one hand on the forehead and lifting the chin up.\n",
      "   - Check for normal breathing for a maximum of 10 seconds.\n",
      "4. **Start CPR**: If the person is not breathing:\n",
      "   - Begin chest compressions at a rate of 30 pushes.\n",
      "   - After 30 compressions, give 2 gentle rescue breaths by tilting the head back, pinching the nose, and breathing into their mouth.\n",
      "   - Continue the cycle of 30 compressions and 2 breaths until medical help arrives or the person wakes up.\n",
      "\n",
      "Always ensure the airway is clear and keep the person warm if they are drowsy or have an altered level of consciousness.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: medium ---\n",
      "Processing question: how to revive a person who is unconscious?\n",
      "--- RETRIEVE ---\n",
      "Using original question: how to revive a person who is unconscious?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: how to revive a person who is unconscious?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 5\n",
      "Workflow type: medium\n",
      "--- MEDIUM WORKFLOW ---\n",
      "Number of documents used: 5, total grades: relevant\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: To revive a person who is unconscious, follow these steps:\n",
      "\n",
      "1. **Check Responsiveness**: Gently shak...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- BASIC OR MEDIUM WORKFLOW ---\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: To revive a person who is unconscious, follow these steps:\n",
      "\n",
      "1. **Check Responsiveness**: Gently shake the person and shout to see if they respond.\n",
      "2. **Call for Help**: If there is no response, call emergency services immediately.\n",
      "3. **Check Breathing**: \n",
      "   - Place the person on their back on a firm surface.\n",
      "   - Tilt their head back by placing one hand on the forehead and lifting the chin up.\n",
      "   - Check for normal breathing for a maximum of 10 seconds.\n",
      "4. **If Not Breathing Normally**: \n",
      "   - Start CPR: \n",
      "     - Push down hard in the center of the chest at a rate of 30 compressions followed by 2 rescue breaths (30:2).\n",
      "     - Continue this cycle until medics arrive or the person wakes up.\n",
      "5. **If Breathing Normally**: \n",
      "   - Place the person in the recovery position (on their side).\n",
      "   - Monitor them continuously and be ready to act if they stop breathing or show signs of distress.\n",
      "\n",
      "Always ensure the airway is clear and keep the person warm.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: advanced ---\n",
      "Processing question: how to revive a person who is unconscious?\n",
      "--- RETRIEVE ---\n",
      "Using original question: how to revive a person who is unconscious?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: how to revive a person who is unconscious?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 5\n",
      "Workflow type: advanced\n",
      "--- ADVANCED WORKFLOW ---\n",
      "Number of documents used: 5, total grades: relevant\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: To revive a person who is unconscious, follow these steps:\n",
      "\n",
      "1. **Check for Breathing**: Ensure the p...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "Grounded: True\n",
      "--- GRADE GENERATION vs QUESTION ---\n",
      "Useful: True\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: To revive a person who is unconscious, follow these steps:\n",
      "\n",
      "1. **Check for Breathing**: Ensure the person is breathing normally. If not, proceed to the next steps.\n",
      "2. **Recovery Position**: If the person is breathing, place them in the recovery position on their side and monitor them continuously.\n",
      "3. **If Not Breathing**: \n",
      "   - Lay the person on their back on a firm surface.\n",
      "   - Tilt their head back by placing one hand on the forehead and lifting the chin up.\n",
      "   - Check for normal breathing for a maximum of 10 seconds.\n",
      "4. **Start CPR**: If the person is not breathing:\n",
      "   - Begin chest compressions at a rate of 30 pushes.\n",
      "   - Follow with 2 rescue breaths: tilt the head back, pinch the nose, and give 2 gentle breaths.\n",
      "   - Continue the cycle of 30 chest compressions and 2 rescue breaths until medical help arrives or the person wakes up.\n",
      "\n",
      "Always ensure to call for emergency medical assistance if someone is unconscious.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: basic ---\n",
      "Processing question: what are the steps for cpr?\n",
      "--- RETRIEVE ---\n",
      "Using original question: what are the steps for cpr?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: what are the steps for cpr?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 5\n",
      "Workflow type: basic\n",
      "--- BASIC WORKFLOW ---\n",
      "Number of documents used: 5, total grades: relevant\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: The steps for CPR are as follows:\n",
      "\n",
      "1. **Check Responsiveness**: Ensure the person is unresponsive.\n",
      "2...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- BASIC OR MEDIUM WORKFLOW ---\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: The steps for CPR are as follows:\n",
      "\n",
      "1. **Check Responsiveness**: Ensure the person is unresponsive.\n",
      "2. **Call for Help**: Ensure help is on the way.\n",
      "3. **Position the Person**: Lay the person on their back on a firm surface.\n",
      "4. **Open the Airway**: Tilt the head back slightly and lift the chin.\n",
      "5. **Check for Breathing**: Look, listen, and feel for breathing for no more than 10 seconds.\n",
      "6. **Start Chest Compressions**:\n",
      "   - Place your hands in the center of the chest.\n",
      "   - Push down hard and fast (at a rate of 100-120 compressions per minute) to a depth of at least 1/3 of the chest depth.\n",
      "   - Perform 30 compressions.\n",
      "7. **Give Rescue Breaths**:\n",
      "   - Pinch the nose, cover the mouth, and give 2 gentle breaths, ensuring the chest rises.\n",
      "8. **Continue the Cycle**: Repeat the cycle of 30 compressions and 2 breaths until help arrives or the person shows signs of life.\n",
      "\n",
      "For infants and children, the technique is similar but adjusted for their size and needs. Always remember to keep the airway open and check for any obstructions if necessary.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: medium ---\n",
      "Processing question: what are the steps for cpr?\n",
      "--- RETRIEVE ---\n",
      "Using original question: what are the steps for cpr?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: what are the steps for cpr?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 4\n",
      "Workflow type: medium\n",
      "--- MEDIUM WORKFLOW ---\n",
      "Number of documents used: 4, total grades: relevant\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: The steps for CPR are as follows:\n",
      "\n",
      "1. **Check Responsiveness**: Ensure the person is unresponsive.\n",
      "2...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- BASIC OR MEDIUM WORKFLOW ---\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: The steps for CPR are as follows:\n",
      "\n",
      "1. **Check Responsiveness**: Ensure the person is unresponsive.\n",
      "2. **Call for Help**: Ensure help is on the way.\n",
      "3. **Position the Person**: Lay the person on their back on a firm surface.\n",
      "4. **Chest Compressions**:\n",
      "   - Kneel beside the person’s chest.\n",
      "   - Place your hands in the middle of the chest.\n",
      "   - Push down hard and fast at a rate of 30 compressions (1, 2, 3, ... 30).\n",
      "   - Compress to at least 1/3 of the chest depth and release completely between compressions.\n",
      "5. **Rescue Breaths**:\n",
      "   - After 30 compressions, give 2 gentle breaths.\n",
      "   - Tilt the head back, lift the chin, pinch the nose, and give the breaths while checking for chest rise.\n",
      "6. **Continue the Cycle**: Repeat the cycle of 30 compressions and 2 breaths until help arrives or the person shows signs of life.\n",
      "\n",
      "Always start CPR with 5 breaths first if the situation allows.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: advanced ---\n",
      "Processing question: what are the steps for cpr?\n",
      "--- RETRIEVE ---\n",
      "Using original question: what are the steps for cpr?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: what are the steps for cpr?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 4\n",
      "Workflow type: advanced\n",
      "--- ADVANCED WORKFLOW ---\n",
      "Number of documents used: 4, total grades: relevant\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: The steps for CPR are as follows:\n",
      "\n",
      "1. **Check Responsiveness**: Ensure the person is unresponsive.\n",
      "2...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "Grounded: True\n",
      "--- GRADE GENERATION vs QUESTION ---\n",
      "Useful: True\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: The steps for CPR are as follows:\n",
      "\n",
      "1. **Check Responsiveness**: Ensure the person is unresponsive.\n",
      "2. **Call for Help**: Ensure help is on the way.\n",
      "3. **Position the Person**: Lay the person on their back on a firm surface.\n",
      "4. **Chest Compressions**:\n",
      "   - Kneel beside the person’s chest.\n",
      "   - Place your hands in the middle of their chest.\n",
      "   - Push down hard and fast at a rate of 30 compressions (1, 2, 3, ... 30).\n",
      "   - Compress to at least 1/3 of the chest depth and release completely between compressions.\n",
      "5. **Rescue Breaths**:\n",
      "   - After 30 compressions, give 2 gentle breaths.\n",
      "   - Tilt the head back with one hand on the forehead and lift the chin with the other hand.\n",
      "   - Pinch the nose and give 2 gentle rescue breaths, ensuring the chest rises with each breath.\n",
      "6. **Continue the Cycle**: Repeat the cycle of 30 compressions and 2 breaths until medical help arrives or the person shows signs of recovery.\n",
      "\n",
      "Always start CPR with 5 breaths first if the situation allows.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: basic ---\n",
      "Processing question: how to treat a burn injury?\n",
      "--- RETRIEVE ---\n",
      "Using original question: how to treat a burn injury?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: how to treat a burn injury?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 1\n",
      "Workflow type: basic\n",
      "--- BASIC WORKFLOW ---\n",
      "Number of documents used: 1, total grades: relevant\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: To treat a burn injury, follow these steps:\n",
      "\n",
      "1. Cool the affected area with tepid water (20 degrees)...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- BASIC OR MEDIUM WORKFLOW ---\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: To treat a burn injury, follow these steps:\n",
      "\n",
      "1. Cool the affected area with tepid water (20 degrees) for at least 20 minutes. Do not use ice.\n",
      "2. Avoid over-cooling; ensure the person does not get cold.\n",
      "3. Remove clothing and jewelry, but do not remove any clothing that is stuck to the skin.\n",
      "4. Cover the injured area with plastic wrap or the cleanest material available (such as compresses, bandages, or textiles).\n",
      "5. Elevate the injured area if possible.\n",
      "6. Use a burns dressing (bandage) if available.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: medium ---\n",
      "Processing question: how to treat a burn injury?\n",
      "--- RETRIEVE ---\n",
      "Using original question: how to treat a burn injury?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: how to treat a burn injury?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 1\n",
      "Workflow type: medium\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using original question: how to treat a burn injury?\n",
      "Rephrased question: What are the recommended first aid and treatment protocols for managing burn injuries?\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- RETRIEVE ---\n",
      "Using rephrased question: What are the recommended first aid and treatment protocols for managing burn injuries?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: What are the recommended first aid and treatment protocols for managing burn injuries?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 1\n",
      "Workflow type: medium\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using rephrased question: What are the recommended first aid and treatment protocols for managing burn injuries?\n",
      "Rephrased question: What are the evidence-based first aid and treatment protocols for managing different degrees of burn injuries?\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- RETRIEVE ---\n",
      "Using rephrased question: What are the evidence-based first aid and treatment protocols for managing different degrees of burn injuries?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: What are the evidence-based first aid and treatment protocols for managing different degrees of burn injuries?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 1\n",
      "Workflow type: medium\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using rephrased question: What are the evidence-based first aid and treatment protocols for managing different degrees of burn injuries?\n",
      "Rephrased question: What are the evidence-based first aid and treatment protocols for managing mild, moderate, and severe burn injuries?\n",
      "--- MAX ITERATIONS (3) REACHED ---\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: To treat a burn injury, follow these steps:\n",
      "\n",
      "1. Cool the affected area with tepid water (20 degrees)...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- BASIC OR MEDIUM WORKFLOW ---\n",
      "--- MAX ITERATIONS REACHED, ENDING ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: To treat a burn injury, follow these steps:\n",
      "\n",
      "1. Cool the affected area with tepid water (20 degrees) for at least 20 minutes. Do not use ice.\n",
      "2. Avoid over-cooling; ensure the person does not get cold.\n",
      "3. Remove clothing and jewelry, but do not remove any clothes that are stuck to the skin.\n",
      "4. Cover the injured area with plastic wrap or the cleanest material available (compresses, bandages, textiles).\n",
      "5. Elevate the injured area if possible.\n",
      "6. Use a burns dressing (bandage) if available.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: advanced ---\n",
      "Processing question: how to treat a burn injury?\n",
      "--- RETRIEVE ---\n",
      "Using original question: how to treat a burn injury?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: how to treat a burn injury?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 1\n",
      "Workflow type: advanced\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using original question: how to treat a burn injury?\n",
      "Rephrased question: What are the recommended first aid and treatment protocols for managing burn injuries?\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- RETRIEVE ---\n",
      "Using rephrased question: What are the recommended first aid and treatment protocols for managing burn injuries?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: What are the recommended first aid and treatment protocols for managing burn injuries?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 1\n",
      "Workflow type: advanced\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using rephrased question: What are the recommended first aid and treatment protocols for managing burn injuries?\n",
      "Rephrased question: What are the evidence-based first aid and treatment protocols for managing different degrees of burn injuries?\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- RETRIEVE ---\n",
      "Using rephrased question: What are the evidence-based first aid and treatment protocols for managing different degrees of burn injuries?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: What are the evidence-based first aid and treatment protocols for managing different degrees of burn injuries?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 1\n",
      "Workflow type: advanced\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using rephrased question: What are the evidence-based first aid and treatment protocols for managing different degrees of burn injuries?\n",
      "Rephrased question: What are the evidence-based first aid and treatment protocols for managing mild, moderate, and severe burn injuries?\n",
      "--- MAX ITERATIONS (3) REACHED ---\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: To treat a burn injury, follow these steps:\n",
      "\n",
      "1. Cool the affected area with tepid water (around 20 d...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "Grounded: True\n",
      "--- GRADE GENERATION vs QUESTION ---\n",
      "Useful: True\n",
      "--- MAX ITERATIONS REACHED, ENDING ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: To treat a burn injury, follow these steps:\n",
      "\n",
      "1. Cool the affected area with tepid water (around 20 degrees Celsius) for at least 20 minutes. Do not use ice.\n",
      "2. Avoid over-cooling; ensure the person does not get cold.\n",
      "3. Remove clothing and jewelry from the area, but do not remove any clothing that is stuck to the skin.\n",
      "4. Cover the injured area with plastic wrap, glad pack, or the cleanest material available (such as compresses, bandages, or textiles).\n",
      "5. Elevate the injured area if possible.\n",
      "6. Use a burns dressing (bandage) if available.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: basic ---\n",
      "Processing question: what to do for someone having a heart attack?\n",
      "--- RETRIEVE ---\n",
      "Using original question: what to do for someone having a heart attack?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: what to do for someone having a heart attack?\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 2\n",
      "Workflow type: basic\n",
      "--- BASIC WORKFLOW ---\n",
      "Number of documents used: 2, total grades: relevant\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: For someone having a heart attack, you should:\n",
      "\n",
      "1. Give the person an aspirin (300 mg powder or 300 ...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- BASIC OR MEDIUM WORKFLOW ---\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: For someone having a heart attack, you should:\n",
      "\n",
      "1. Give the person an aspirin (300 mg powder or 300 – 500 mg tablet to chew) immediately, unless they have an allergy to aspirin, other salicylates, or NSAIDs.\n",
      "2. If the person has angina, provide their angina medication (GTN tablet or spray) every 5 minutes as needed until the pain is relieved, but do not exceed 3 tablets under the tongue every 15 minutes.\n",
      "3. Ensure help is on the way and keep the phone free for any follow-up calls.\n",
      "4. Help the person to get comfortable and reassure them, keeping the environment calm and quiet.\n",
      "5. Monitor the person closely and report any changes immediately.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: medium ---\n",
      "Processing question: what to do for someone having a heart attack?\n",
      "--- RETRIEVE ---\n",
      "Using original question: what to do for someone having a heart attack?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: what to do for someone having a heart attack?\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 2\n",
      "Workflow type: medium\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using original question: what to do for someone having a heart attack?\n",
      "Rephrased question: What are the immediate steps to take when assisting someone experiencing a heart attack?\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- RETRIEVE ---\n",
      "Using rephrased question: What are the immediate steps to take when assisting someone experiencing a heart attack?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: What are the immediate steps to take when assisting someone experiencing a heart attack?\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 1\n",
      "Workflow type: medium\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using rephrased question: What are the immediate steps to take when assisting someone experiencing a heart attack?\n",
      "Rephrased question: What are the critical first aid steps to follow when helping someone who is having a heart attack?\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- RETRIEVE ---\n",
      "Using rephrased question: What are the critical first aid steps to follow when helping someone who is having a heart attack?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: What are the critical first aid steps to follow when helping someone who is having a heart attack?\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: not_relevant, Number of documents: 0\n",
      "Workflow type: medium\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using rephrased question: What are the critical first aid steps to follow when helping someone who is having a heart attack?\n",
      "Rephrased question: What are the essential first aid procedures to administer during a heart attack emergency?\n",
      "--- MAX ITERATIONS (3) REACHED ---\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: I don't know....\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- BASIC OR MEDIUM WORKFLOW ---\n",
      "--- MAX ITERATIONS REACHED, ENDING ---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Final Output: I don't know.\n",
      "-------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "--- Running workflow: advanced ---\n",
      "Processing question: what to do for someone having a heart attack?\n",
      "--- RETRIEVE ---\n",
      "Using original question: what to do for someone having a heart attack?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: what to do for someone having a heart attack?\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 2\n",
      "Workflow type: advanced\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using original question: what to do for someone having a heart attack?\n",
      "Rephrased question: What are the immediate steps to take when someone is experiencing a heart attack?\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- RETRIEVE ---\n",
      "Using rephrased question: What are the immediate steps to take when someone is experiencing a heart attack?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: What are the immediate steps to take when someone is experiencing a heart attack?\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 1\n",
      "Workflow type: advanced\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using rephrased question: What are the immediate steps to take when someone is experiencing a heart attack?\n",
      "Rephrased question: What are the critical first aid steps to follow when someone is having a heart attack?\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- RETRIEVE ---\n",
      "Using rephrased question: What are the critical first aid steps to follow when someone is having a heart attack?\n",
      "node 3: ## emergency response\n",
      "Retrieved 5 documents\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "Grading 5 documents for relevance to question: What are the critical first aid steps to follow when someone is having a heart attack?\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT NOT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 2\n",
      "Workflow type: advanced\n",
      "--- DECISION: DOCUMENTS ARE NOT RELEVANT, TRANSFORM QUERY ---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "--- TRANSFORM QUERY ---\n",
      "Using rephrased question: What are the critical first aid steps to follow when someone is having a heart attack?\n",
      "Rephrased question: What are the essential first aid procedures to administer during a heart attack emergency?\n",
      "--- MAX ITERATIONS (3) REACHED ---\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "--- GENERATE ---\n",
      "Generated answer: I don't know....\n",
      "--- CHECK HALLUCINATIONS ---\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'detected': False, 'filtered': False}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     37\u001b[39m inputs = {\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question,\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mllm\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mworkflow_type\u001b[39m\u001b[33m\"\u001b[39m: workflow,  \u001b[38;5;66;03m# Change to \"deep\" for deeper workflows\u001b[39;00m\n\u001b[32m     44\u001b[39m }\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Run the workflow\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Node\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNode \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1670\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   1664\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   1665\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   1666\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   1667\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   1668\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   1669\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m1670\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   1677\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:231\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    229\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:464\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    462\u001b[39m             \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    463\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:226\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    225\u001b[39m     context.run(_set_config_context, config)\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse:\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langgraph\\graph\\graph.py:95\u001b[39m, in \u001b[36mBranch._route\u001b[39m\u001b[34m(self, input, config, reader, writer)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     94\u001b[39m     value = \u001b[38;5;28minput\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._finish(writer, \u001b[38;5;28minput\u001b[39m, result, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:218\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m     context = copy_context()\n\u001b[32m    217\u001b[39m     context.run(_set_config_context, child_config)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 294\u001b[39m, in \u001b[36mgrade_generation_v_documents_and_question\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    286\u001b[39m hallucination_prompt = ChatPromptTemplate.from_messages([\n\u001b[32m    287\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\"\"\u001b[39m\u001b[33mYou are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[33m    Give a binary score \u001b[39m\u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m'\u001b[39m\u001b[33mYes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m means that the answer is grounded in / supported by the set of facts.\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[33m    Provide the binary score as a JSON with a single key \u001b[39m\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and no preamble or explanation.\u001b[39m\u001b[33m\"\"\"\u001b[39m),\n\u001b[32m    290\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSet of facts: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{documents}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m LLM generation: \u001b[39m\u001b[38;5;132;01m{generation}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    291\u001b[39m ])\n\u001b[32m    293\u001b[39m hallucination_chain = hallucination_prompt | model | StrOutputParser()\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m grade = \u001b[43mhallucination_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgeneration\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    300\u001b[39m     grade_dict = json.loads(grade)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3022\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3020\u001b[39m             \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3021\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3022\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3023\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3024\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    277\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    281\u001b[39m     **kwargs: Any,\n\u001b[32m    282\u001b[39m ) -> BaseMessage:\n\u001b[32m    283\u001b[39m     config = ensure_config(config)\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    285\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    779\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    780\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    783\u001b[39m     **kwargs: Any,\n\u001b[32m    784\u001b[39m ) -> LLMResult:\n\u001b[32m    785\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    642\u001b[39m             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    644\u001b[39m flattened_outputs = [\n\u001b[32m    645\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    647\u001b[39m ]\n\u001b[32m    648\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    632\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m         )\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    641\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    849\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    850\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    854\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    855\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:781\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    779\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:929\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    886\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    888\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    926\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    927\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    928\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1276\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1264\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1271\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1272\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1273\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1274\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1275\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\openai\\_base_client.py:949\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1057\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1054\u001b[39m         err.response.read()\n\u001b[32m   1056\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1060\u001b[39m     cast_to=cast_to,\n\u001b[32m   1061\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1065\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1066\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'detected': False, 'filtered': False}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}",
      "During task with name 'generate' and id '07799cf3-c215-6555-5d7f-6b06ae6b52cf'"
     ]
    }
   ],
   "source": [
    "\n",
    "app = build_rag_workflowv1()\n",
    "\n",
    "# # Initial state\n",
    "# inputs = {\n",
    "#     \"question\": \"what to do for someone having a heart attack?\",\n",
    "#     \"llm\": model,\n",
    "#     \"retriever_type\": \"knowledge_graph\",  # Change to \"knowledge_graph\" or \"hybrid\" or \"vector_store\" as needed\n",
    "#     \"load_persist\": \"./kgstore\",\n",
    "#     \"nodes\": nodes,  # Use the processed nodes from markdownParser\n",
    "#     \"workflow_type\": \"basic\",  # Change to \"deep\" for deeper workflows\n",
    "# }\n",
    "\n",
    "# # Run the workflow\n",
    "# for output in app.stream(inputs):\n",
    "#     for key, value in output.items():\n",
    "#         # Node\n",
    "#         print(f\"Node '{key}':\")\n",
    "\n",
    "        \n",
    "#     print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "# # Final output\n",
    "# print(f\"Final Output: {value['generation']}\")\n",
    "# print(f\"-------------------------------------------------------\\n\")\n",
    "# print(f\"-------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "workflow_type = [\"basic\", \"medium\", \"advanced\"]  # Change to \"basic\", \"medium\", or \"advanced\" as needed\n",
    "\n",
    "for question in test_questions:\n",
    "    for workflow in workflow_type:\n",
    "        print(f\"\\n--- Running workflow: {workflow} ---\")\n",
    "        \n",
    "        print(f\"Processing question: {question}\")\n",
    "        # Initial state\n",
    "        inputs = {\n",
    "            \"question\": question,\n",
    "            \"llm\": model,\n",
    "            \"retriever_type\": \"vector_store\",  # Change to \"knowledge_graph\" or \"hybrid\" as needed\n",
    "            \"load_persist\": \"./kgstore\",\n",
    "            \"nodes\": nodes,  # Use the processed nodes from markdownParser\n",
    "            \"workflow_type\": workflow,  # Change to \"deep\" for deeper workflows\n",
    "        }\n",
    "\n",
    "        # Run the workflow\n",
    "        for output in app.stream(inputs):\n",
    "            for key, value in output.items():\n",
    "                # Node\n",
    "                print(f\"Node '{key}':\")\n",
    "\n",
    "                \n",
    "            print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "        # Final output\n",
    "        print(f\"Final Output: {value['generation']}\")\n",
    "        print(f\"-------------------------------------------------------\\n\")\n",
    "        print(f\"-------------------------------------------------------\\n\")\n",
    "\n",
    "        # from collections import Counter\n",
    "\n",
    "        # doc_sources = []\n",
    "        # for doc in value[\"documents\"]:\n",
    "        #     # print(f\"Source: {doc.metadata.get('source', 'No source available')}\")  # Print source if available\n",
    "        #     doc_sources.append(doc.metadata.get(\"source\", \"No source available\"))\n",
    "        # value[\"documents\"][1].metadata.get(\"source\", \"No source available\")\n",
    "        # Count occurrences of each source type\n",
    "        # source_counts = Counter(doc_sources)\n",
    "        # print(\"No of docs from vector store:\", source_counts.get(\"vector\", 0))\n",
    "        # print(\"No of docs from knowledge_graph:\", source_counts.get(\"knowledge_graph\", 0))\n",
    "        # print(\"No of docs with no source available:\", source_counts.get(\"No source available\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_retriever(question=\"how to treat a burn injury?\", nodes=nodes, llm=model, type=\"vector_store\")\n",
    "documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
