{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4414b4f",
   "metadata": {},
   "source": [
    "#### How to run this notebook\n",
    "For simplicity, just run all cells\n",
    "- open browser and go to: localhost:7863\n",
    "- since the actual KG data is large, i cant upload to the github, i have created smaller kg_test with only 50 nodes.[first 2 chapters from the norwegian index.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925a642",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '.venv (Python -1.-1.-1)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from src.model import get_llamaindex_model, get_llamaindex_model_mini, get_huggingface_embedding_model\n",
    "from llama_index.core import Settings\n",
    "from src import get_azure_openai_model, get_azure_openai_chat_model, get_azure_openai_mini_model\n",
    "from src.parser import markdownParser\n",
    "from src import build_rag_workflow\n",
    "\n",
    "llm = get_llamaindex_model_mini()\n",
    "\n",
    "llm2 = get_llamaindex_model()\n",
    "\n",
    "embed_model = get_huggingface_embedding_model()\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "model = get_azure_openai_chat_model()\n",
    "\n",
    "\n",
    "nodes = markdownParser(input_dir=\"./kgdata/\")\n",
    "print(f\"Processed {len(nodes)} nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5233c7",
   "metadata": {},
   "source": [
    "#### Final UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting concurrent streaming for: how to help a person injured in motorbike accident...\n",
      "Starting vector_store retriever...\n",
      "Starting knowledge_graph retriever...\n",
      "Starting hybrid retriever...\n",
      "--- RETRIEVE ---\n",
      "Using original question: how to help a person injured in motorbike accident?\n",
      "node 3: ## emergency response\n",
      "--- RETRIEVE ---\n",
      "Using original question: how to help a person injured in motorbike accident?\n",
      "node 3: ## emergency response\n",
      "🔄 Loading persisted knowledge graph from ./kgstore\n",
      "--- RETRIEVE ---\n",
      "Using original question: how to help a person injured in motorbike accident?\n",
      "node 3: ## emergency response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-523' coro=<Queue.start_processing() running at c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\gradio\\queueing.py:308> wait_for=<Future finished result=None>>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-514' coro=<_delete_state() running at c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\gradio\\route_utils.py:935> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-524' coro=<Queue.start_progress_updates() running at c:\\Users\\newac\\OneDrive\\Desktop\\Master\\.venv\\Lib\\site-packages\\gradio\\queueing.py:358> wait_for=<Future pending cb=[Task.__wakeup()]>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading persisted knowledge graph from ./kgstore\n",
      "Retrieved 30 documents\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "--- FAST WORKFLOW ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: , Number of documents: 30\n",
      "--- DECISION: DOCUMENTS ARE RELEVANT, GENERATE ANSWER ---\n",
      "--- GENERATE ---\n",
      "✅ Successfully loaded persisted knowledge graph!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded persisted knowledge graph!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: To help a person injured in a motorbike accident, follow these steps:\n",
      "\n",
      "1. **Ensure Safety**: Make su...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- FAST WORKFLOW ---\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed vector_store retriever\n",
      "✅ vector_store completed in 38.35s (1/3)\n",
      "Vector Store Retrieved 30 documents\n",
      "Knowledge Graph Retrieved 19 documents\n",
      "Retrieved 49 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "--- FAST WORKFLOW ---\n",
      "Retrieved 19 documents\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: , Number of documents: 49\n",
      "--- DECISION: DOCUMENTS ARE RELEVANT, GENERATE ANSWER ---\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "--- FAST WORKFLOW ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: , Number of documents: 19\n",
      "--- DECISION: DOCUMENTS ARE RELEVANT, GENERATE ANSWER ---\n",
      "--- GENERATE ---\n",
      "--- GENERATE ---\n",
      "Generated answer: To help a person injured in a motorbike accident, follow these steps:\n",
      "\n",
      "1. **Ensure Safety**: Make su...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- FAST WORKFLOW ---\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Completed hybrid retriever\n",
      "✅ hybrid completed in 46.22s (2/3)\n",
      "Generated answer: I don't know....\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "--- FAST WORKFLOW ---\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Completed knowledge_graph retriever\n",
      "✅ knowledge_graph completed in 96.20s (3/3)\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pprint\n",
    "from io import StringIO\n",
    "import sys\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def build_rag_interface(build_rag_workflowv1, model, nodes):\n",
    "    \"\"\"\n",
    "    Creates a Gradio interface for the RAG workflow\n",
    "    \n",
    "    Args:\n",
    "        build_rag_workflowv1: Function that builds the RAG workflow\n",
    "        model: The language model to use\n",
    "        nodes: Processed nodes from markdownParser\n",
    "    \"\"\"\n",
    "    \n",
    "    def run_single_retriever(question, retriever_type, workflow_type, load_persist_path):\n",
    "        \"\"\"Run workflow for a single retriever type\"\"\"\n",
    "        try:\n",
    "            print(f\"Starting {retriever_type} retriever...\")\n",
    "            \n",
    "            # Build the RAG workflow\n",
    "            app = build_rag_workflowv1()\n",
    "            \n",
    "            # Set up inputs\n",
    "            inputs = {\n",
    "                \"question\": question,\n",
    "                \"llm\": model,\n",
    "                \"retriever_type\": retriever_type,\n",
    "                \"load_persist\": load_persist_path,\n",
    "                \"nodes\": nodes,\n",
    "                \"workflow_type\": workflow_type,\n",
    "            }\n",
    "            \n",
    "            # Run the workflow and capture outputs\n",
    "            final_value = None\n",
    "            for output in app.stream(inputs):\n",
    "                for key, value in output.items():\n",
    "                    final_value = value\n",
    "            \n",
    "            # Format results\n",
    "            if final_value:\n",
    "                response = final_value.get('generation', 'No response generated')\n",
    "                # documents = final_value.get('documents', [])\n",
    "                \n",
    "                # # Format documents\n",
    "                # formatted_docs = []\n",
    "                # for i, doc in enumerate(documents):\n",
    "                #     header_path = doc.node.metadata.get('header_path', 'No header path available')\n",
    "                #     text_content = doc.node.text\n",
    "                #     source = doc.node.metadata.get('source', 'unknown')\n",
    "                #     score = getattr(doc, 'score', 'N/A')\n",
    "                    \n",
    "#                     formatted_doc = f\"\"\"**Document {i+1}** (Score: {score})\n",
    "# **Header Path:** {header_path}\n",
    "# **Source:** {source}\n",
    "# **Content:**\n",
    "# {text_content}\n",
    "# {\"=\"*80}\n",
    "# \"\"\"\n",
    "#                     formatted_docs.append(formatted_doc)\n",
    "                \n",
    "#                 doc_text = \"\\n\".join(formatted_docs) if formatted_docs else \"No documents retrieved.\"\n",
    "#                 doc_stats = f\"**{retriever_type.replace('_', ' ').title()} - {len(documents)} Documents Retrieved**\\n\\n\"\n",
    "                \n",
    "                result = f\"**✅ RESPONSE:**\\n{response}\" #\\n\\n**📋 DOCUMENTS:**\\n{doc_stats}{doc_text}\"\n",
    "                print(f\"Completed {retriever_type} retriever\")\n",
    "                return result\n",
    "            else:\n",
    "                return f\"**❌ ERROR:** No response generated for {retriever_type}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"**❌ ERROR in {retriever_type}:** {str(e)}\"\n",
    "            print(f\"Error in {retriever_type}: {str(e)}\")\n",
    "            return error_msg\n",
    "\n",
    "    def process_question_streaming(question, workflow_type, load_persist_path):\n",
    "        \"\"\"Process question with real-time streaming updates as results become available\"\"\"\n",
    "        \n",
    "        if not question.strip():\n",
    "            yield \"Please enter a question.\", \"Please enter a question.\", \"Please enter a question.\"\n",
    "            return\n",
    "        \n",
    "        print(f\"🚀 Starting concurrent streaming for: {question[:50]}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize with processing messages  \n",
    "        results = {\n",
    "            'vector_store': \"🔄 **VECTOR STORE** - Processing... Please wait.\",\n",
    "            'knowledge_graph': \"🔄 **KNOWLEDGE GRAPH** - Processing... Please wait.\", \n",
    "            'hybrid': \"🔄 **HYBRID** - Processing... Please wait.\"\n",
    "        }\n",
    "        \n",
    "        # Yield initial state immediately\n",
    "        yield results['vector_store'], results['knowledge_graph'], results['hybrid']\n",
    "        \n",
    "        # Create thread-safe variables\n",
    "        results_lock = threading.Lock()\n",
    "        completion_times = {}\n",
    "        completed_count = 0\n",
    "        \n",
    "        def process_single_result(retriever_type, question, workflow_type, load_persist_path):\n",
    "            \"\"\"Process a single retriever and return the result\"\"\"\n",
    "            thread_start = time.time()\n",
    "            try:\n",
    "                result = run_single_retriever(question, retriever_type, workflow_type, load_persist_path)\n",
    "                exec_time = time.time() - thread_start\n",
    "                enhanced_result = f\"⚡ **Completed in {exec_time:.2f}s** | **{retriever_type.replace('_', ' ').title()}**\\n\\n{result}\"\n",
    "                return retriever_type, enhanced_result, exec_time, True\n",
    "            except Exception as e:\n",
    "                exec_time = time.time() - thread_start\n",
    "                error_result = f\"❌ **FAILED in {exec_time:.2f}s** | **{retriever_type.replace('_', ' ').title()}**\\n\\n**Error:** {str(e)}\"\n",
    "                return retriever_type, error_result, exec_time, False\n",
    "        \n",
    "        # Start all three workflows concurrently\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            # Submit all tasks\n",
    "            futures = {\n",
    "                executor.submit(process_single_result, 'vector_store', question, workflow_type, load_persist_path): 'vector_store',\n",
    "                executor.submit(process_single_result, 'knowledge_graph', question, workflow_type, load_persist_path): 'knowledge_graph',\n",
    "                executor.submit(process_single_result, 'hybrid', question, workflow_type, load_persist_path): 'hybrid'\n",
    "            }\n",
    "            \n",
    "            # Process results as they complete and yield immediately\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    retriever_type, result, exec_time, success = future.result()\n",
    "                    \n",
    "                    with results_lock:\n",
    "                        completed_count += 1\n",
    "                        results[retriever_type] = result\n",
    "                        if success:\n",
    "                            completion_times[retriever_type] = exec_time\n",
    "                        \n",
    "                        print(f\"✅ {retriever_type} completed in {exec_time:.2f}s ({completed_count}/3)\")\n",
    "                    \n",
    "                    # Yield updated results immediately - this is the key!\n",
    "                    yield results['vector_store'], results['knowledge_graph'], results['hybrid']\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Future execution error: {str(e)}\")\n",
    "                    # Still yield the current state even if there's an error\n",
    "                    yield results['vector_store'], results['knowledge_graph'], results['hybrid']\n",
    "        \n",
    "        # # Final processing - add trophy to fastest result\n",
    "        # if completion_times:\n",
    "        #     fastest_time = min(completion_times.values())\n",
    "        #     fastest_retriever = min(completion_times.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "        #     with results_lock:\n",
    "        #         if fastest_retriever in results:\n",
    "        #             results[fastest_retriever] = f\"🏆 **FASTEST** | {results[fastest_retriever]}\"\n",
    "        \n",
    "        # total_time = time.time() - start_time\n",
    "        # print(f\"🎉 All retrievers completed! Total time: {total_time:.2f}s\")\n",
    "        \n",
    "        # Final yield with trophy added\n",
    "        yield results['vector_store'], results['knowledge_graph'], results['hybrid']\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    with gr.Blocks(title=\"RAG Workflow Interface\", theme=gr.themes.Soft()) as interface:\n",
    "        \n",
    "        gr.Markdown(\"# Medical emergency RAG Workflow Interface\")\n",
    "        gr.Markdown(\"This is a Retrieval-Augmented Generation (RAG) interface for medical emergencies. This interface allows you to ask questions related to medical emergencies and get responses using a combination of retrieval and generation techniques.\")\n",
    "        gr.Markdown(\"**⚡ Concurrent Processing:** When you click 'Process Question', the system will automatically run all three retriever types (Vector Store, Knowledge Graph, and Hybrid) **in parallel** for faster results.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                # Input section\n",
    "                question_input = gr.Textbox(\n",
    "                    label=\"Question\",\n",
    "                    placeholder=\"Enter your question here...\",\n",
    "                    lines=3,\n",
    "                    value=\"how to revive a person who is unconscious\"\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    workflow_type = gr.Dropdown(\n",
    "                        choices=[\"fast\", \"deep\"],\n",
    "                        value=\"fast\",\n",
    "                        label=\"Workflow Type\"\n",
    "                    )\n",
    "                \n",
    "                load_persist_path = gr.Textbox(\n",
    "                    label=\"Knowledge Store Path\",\n",
    "                    value=\"./kgstore_test_50\",\n",
    "                    placeholder=\"Path to knowledge store\"\n",
    "                )\n",
    "                \n",
    "                submit_btn = gr.Button(\"Process Question\", variant=\"primary\")\n",
    "                clear_btn = gr.Button(\"Clear\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.Column(scale=3):\n",
    "                # Output section\n",
    "                gr.Markdown(\"### Results\")\n",
    "                \n",
    "                with gr.Tabs():\n",
    "                    with gr.TabItem(\"Vector Store\"):\n",
    "                        vector_output = gr.Markdown(\n",
    "                            label=\"Vector Store Results\",\n",
    "                            # lines=20,\n",
    "                            min_height=200,\n",
    "                            max_height=500,\n",
    "                            show_copy_button=True\n",
    "                        )\n",
    "                    \n",
    "                    with gr.TabItem(\"Knowledge Graph\"):\n",
    "                        kg_output = gr.Markdown(\n",
    "                            label=\"Knowledge Graph Results\",\n",
    "                            # lines=20,\n",
    "                            min_height=200,\n",
    "                            max_height=500,\n",
    "                            show_copy_button=True\n",
    "                        )\n",
    "                    \n",
    "                    with gr.TabItem(\"Hybrid\"):\n",
    "                        hybrid_output = gr.Markdown(\n",
    "                            label=\"Hybrid Results\",\n",
    "                            # lines=20,\n",
    "                            min_height=200,\n",
    "                            max_height=500,\n",
    "                            show_copy_button=True\n",
    "                        )\n",
    "        \n",
    "        # Event handlers\n",
    "        submit_btn.click(\n",
    "            fn=process_question_streaming,\n",
    "            inputs=[question_input, workflow_type, load_persist_path],\n",
    "            outputs=[vector_output, kg_output, hybrid_output]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=lambda: (\"\", \"\", \"\", \"\", \"fast\", \"./kgstore\"),\n",
    "            outputs=[question_input, vector_output, kg_output, hybrid_output, workflow_type, load_persist_path]\n",
    "        )\n",
    "        \n",
    "        # Example questions\n",
    "        gr.Markdown(\"### Example Questions\")\n",
    "        example_questions = [\n",
    "            \"how to revive a person who is unconscious\",\n",
    "            \"what are the steps for CPR?\",\n",
    "            \"how to treat a burn injury?\",\n",
    "            \"what to do in case of choking?\",\n",
    "            \"how to help a person injured in motorbike accident?\",\n",
    "            \"what are the symptoms of a heart attack?\",\n",
    "        ]\n",
    "        \n",
    "        for i, example in enumerate(example_questions):\n",
    "            gr.Button(example, size=\"sm\").click(\n",
    "                fn=lambda x=example: x,\n",
    "                outputs=question_input\n",
    "            )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Usage example (you would call this in your main script):\n",
    "\n",
    "# Assuming you have your components ready:\n",
    "# - build_rag_workflowv1: your workflow builder function\n",
    "# - model: your language model\n",
    "# - nodes: your processed nodes\n",
    "\n",
    "interface = build_rag_interface(build_rag_workflow, model, nodes)\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    interface.launch(\n",
    "        server_name=\"0.0.0.0\",  # Allow external access\n",
    "        server_port=7863,       # Default Gradio port\n",
    "        share=False,            # Set to True for public sharing\n",
    "        debug=True              # Enable debug mode\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
