{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784de821",
   "metadata": {},
   "source": [
    "#### How to run this notebook\n",
    "For simplicity, just run all cells\n",
    "- open browser and go to: localhost:7863\n",
    "- since the actual KG data is large, i cant upload to the github, i have created smaller kg_test with only 50 nodes.[first 2 chapters from the norwegian index.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98d2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\newac\\OneDrive\\Desktop\\Master\\final_structure\\src\\__init__.py:16: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from .DAG_creator import build_rag_workflow\n"
     ]
    }
   ],
   "source": [
    "from src.model import get_llamaindex_model, get_llamaindex_model_mini, get_huggingface_embedding_model\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = get_llamaindex_model_mini()\n",
    "\n",
    "llm2 = get_llamaindex_model()\n",
    "\n",
    "embed_model = get_huggingface_embedding_model()\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5b932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import get_azure_openai_model, get_azure_openai_chat_model, get_azure_openai_mini_model\n",
    "\n",
    "model = get_azure_openai_chat_model()\n",
    "model_mini = get_azure_openai_mini_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d93b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 945 nodes.\n"
     ]
    }
   ],
   "source": [
    "from src.parser import markdownParser\n",
    "nodes = markdownParser(input_dir=\"./kgdata/\")\n",
    "print(f\"Processed {len(nodes)} nodes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[5]  # Display a slice of the nodes to verify processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ade5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.graphflow_v2 import build_emergency_rag_workflowv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676aba52",
   "metadata": {},
   "source": [
    "##### Possible UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f3347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Starting Emergency RAG processing for: how to revive a person who is unconscious...\n",
      "Starting vector_store retriever...\n",
      "Starting knowledge_graph retriever...\n",
      "Starting hybrid retriever...\n",
      "--- DETECT EMERGENCY CONTEXT ---\n",
      "--- DETECT EMERGENCY CONTEXT ---\n",
      "--- DETECT EMERGENCY CONTEXT ---\n",
      "Emergency detected: True, Type: medical\n",
      "Emergency detected: True, Type: medical\n",
      "Emergency detected: True, Type: medical\n",
      "--- ROUTING TO EMERGENCY ANALYSIS ---\n",
      "--- ROUTING TO EMERGENCY ANALYSIS ---\n",
      "--- ROUTING TO EMERGENCY ANALYSIS ---\n",
      "--- ANALYZE EMERGENCY CALL ---\n",
      "--- ANALYZE EMERGENCY CALL ---\n",
      "--- ANALYZE EMERGENCY CALL ---\n",
      "--- GENERATE EMERGENCY REPORT ---\n",
      "--- GENERATE EMERGENCY REPORT ---\n",
      "--- GENERATE EMERGENCY REPORT ---\n",
      "emergency summary: Medical emergency - unconscious person\n",
      "--- RETRIEVE ---\n",
      "Using original question: Medical emergency - unconscious person + Unconsciousness\n",
      "Emergency context: True\n",
      "final question for retrieval: Medical emergency - unconscious person + Unconsciousness\n",
      "emergency summary: Medical emergency - unconscious person\n",
      "emergency summary: Medical emergency - unconscious person\n",
      "--- RETRIEVE ---\n",
      "Using original question: Medical emergency - unconscious person + Unconsciousness\n",
      "Emergency context: True\n",
      "final question for retrieval: Medical emergency - unconscious person + Unconsciousness\n",
      "--- RETRIEVE ---\n",
      "Using original question: Medical emergency - unconscious person + Unconsciousness, unsure if breathing.\n",
      "Emergency context: True\n",
      "final question for retrieval: Medical emergency - unconscious person + Unconsciousness, unsure if breathing.\n",
      "üîÑ Loading persisted knowledge graph from ./kgstore\n",
      "üîÑ Loading persisted knowledge graph from ./kgstore\n",
      "Retrieved 15 documents\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "‚úÖ Successfully loaded persisted knowledge graph!\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "‚úÖ Successfully loaded persisted knowledge graph!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in vector_store: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "‚úÖ vector_store completed in 47.09s (1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Retrieved 15 documents\n",
      "Knowledge Graph Retrieved 11 documents\n",
      "Retrieved 26 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:04<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 13 documents\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "--- CHECK DOCUMENT RELEVANCE TO QUESTION ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "Error in hybrid: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "‚úÖ hybrid completed in 72.40s (2/3)\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- GRADE: DOCUMENT RELEVANT ---\n",
      "--- ASSESS GRADED DOCUMENTS ---\n",
      "Grade: relevant, Number of documents: 13, Emergency: True\n",
      "--- DECISION: SUFFICIENT DOCUMENTS (13 >= 3), GENERATE ANSWER ---\n",
      "--- GENERATE ---\n",
      "Generated answer: If you encounter an unconscious person, follow these steps:\n",
      "\n",
      "1. **Check Responsiveness**: Gently sha...\n",
      "--- CHECK HALLUCINATIONS ---\n",
      "skipping hallucination check for emergency context\n",
      "Useful: yes\n",
      "--- DECISION: GENERATION IS GROUNDED AND USEFUL ---\n",
      "Completed knowledge_graph retriever\n",
      "‚úÖ knowledge_graph completed in 111.07s (3/3)\n",
      "üéâ All emergency retrievers completed! Total time: 111.08s\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pprint\n",
    "from io import StringIO\n",
    "import sys\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "\n",
    "def build_emergency_rag_interface(build_emergency_rag_workflow, model, nodes):\n",
    "    \"\"\"\n",
    "    Creates a Gradio interface for the Emergency Dispatch + RAG workflow\n",
    "    \n",
    "    Args:\n",
    "        build_emergency_rag_workflow: Function that builds the emergency RAG workflow\n",
    "        model: The language model to use\n",
    "        nodes: Processed nodes from markdownParser\n",
    "    \"\"\"\n",
    "    \n",
    "    def run_single_retriever(question, call_transcript, retriever_type, load_persist_vector, load_persist_kg):\n",
    "        \"\"\"Run emergency RAG workflow for a single retriever type\"\"\"\n",
    "        try:\n",
    "            print(f\"Starting {retriever_type} retriever...\")\n",
    "            \n",
    "            # Build the emergency RAG workflow\n",
    "            app = build_emergency_rag_workflow()\n",
    "            \n",
    "            # Set up inputs for emergency RAG\n",
    "            inputs = {\n",
    "                \"question\": question,\n",
    "                \"call_transcript\": call_transcript if call_transcript.strip() else None,\n",
    "                \"llm\": model,\n",
    "                \"retriever_type\": retriever_type,\n",
    "                \"load_persist_vector\": load_persist_vector,\n",
    "                \"load_persist_kg\": load_persist_kg,\n",
    "                \"nodes\": nodes,\n",
    "                \"max_iterations\": 2\n",
    "            }\n",
    "            \n",
    "            # Run the workflow and capture outputs\n",
    "            final_value = None\n",
    "            for output in app.stream(inputs):\n",
    "                for key, value in output.items():\n",
    "                    final_value = value\n",
    "            \n",
    "            # Format results\n",
    "            if final_value:\n",
    "                # Get basic RAG response\n",
    "                response = final_value.get('generation', 'No response generated')\n",
    "#                 documents = final_value.get('documents', [])\n",
    "                \n",
    "#                 # Get emergency-specific data\n",
    "#                 is_emergency = final_value.get('is_emergency', False)\n",
    "#                 emergency_analysis = final_value.get('emergency_analysis', {})\n",
    "#                 structured_report = final_value.get('structured_report', {})\n",
    "#                 emergency_type = final_value.get('emergency_type', 'none')\n",
    "                \n",
    "#                 # Format documents\n",
    "#                 formatted_docs = []\n",
    "#                 for i, doc in enumerate(documents):\n",
    "#                     header_path = doc.node.metadata.get('header_path', 'No header path available')\n",
    "#                     text_content = doc.node.text\n",
    "#                     source = doc.node.metadata.get('source', 'unknown')\n",
    "#                     score = getattr(doc, 'score', 'N/A')\n",
    "                    \n",
    "#                     formatted_doc = f\"\"\"**Document {i+1}** (Score: {score})\n",
    "# **Header Path:** {header_path}\n",
    "# **Source:** {source}\n",
    "# **Content:**\n",
    "# {text_content}\n",
    "# {\"=\"*80}\n",
    "# \"\"\"\n",
    "#                     formatted_docs.append(formatted_doc)\n",
    "                \n",
    "#                 doc_text = \"\\n\".join(formatted_docs) if formatted_docs else \"No documents retrieved.\"\n",
    "                \n",
    "                # Build comprehensive result\n",
    "                result_parts = []\n",
    "                \n",
    "                # # Emergency Analysis Section\n",
    "                # if is_emergency and emergency_analysis:\n",
    "                #     result_parts.append(\"üö® **EMERGENCY DETECTED** üö®\")\n",
    "                #     result_parts.append(f\"**Emergency Type:** {emergency_type}\")\n",
    "                #     result_parts.append(\"**Emergency Analysis:**\")\n",
    "                #     result_parts.append(json.dumps(emergency_analysis, indent=2))\n",
    "                #     result_parts.append(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "                \n",
    "                # # Structured Report Section\n",
    "                # if structured_report:\n",
    "                #     result_parts.append(\"üìã **STRUCTURED EMERGENCY REPORT:**\")\n",
    "                #     result_parts.append(json.dumps(structured_report, indent=2))\n",
    "                #     result_parts.append(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "                \n",
    "                # RAG Response Section\n",
    "                result_parts.append(\"ü§ñ **RAG RESPONSE:**\")\n",
    "                result_parts.append(response)\n",
    "                result_parts.append(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "                \n",
    "                # # Documents Section\n",
    "                # result_parts.append(f\"üìö **RETRIEVED DOCUMENTS ({len(documents)}):**\")\n",
    "                # result_parts.append(doc_text)\n",
    "                \n",
    "                final_result = \"\\n\\n\".join(result_parts)\n",
    "                print(f\"Completed {retriever_type} retriever\")\n",
    "                return final_result\n",
    "            else:\n",
    "                return f\"**‚ùå ERROR:** No response generated for {retriever_type}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"**‚ùå ERROR in {retriever_type}:** {str(e)}\"\n",
    "            print(f\"Error in {retriever_type}: {str(e)}\")\n",
    "            return error_msg\n",
    "\n",
    "    def process_emergency_question_streaming(question, call_transcript, load_persist_vector, load_persist_kg):\n",
    "        \"\"\"Process emergency question with real-time streaming updates\"\"\"\n",
    "        \n",
    "        if not question.strip():\n",
    "            yield \"Please enter a question.\", \"Please enter a question.\", \"Please enter a question.\"\n",
    "            return\n",
    "        \n",
    "        print(f\"üö® Starting Emergency RAG processing for: {question[:50]}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize with processing messages  \n",
    "        results = {\n",
    "            'vector_store': \"üîÑ **VECTOR STORE** - Processing emergency query... Please wait.\",\n",
    "            'knowledge_graph': \"üîÑ **KNOWLEDGE GRAPH** - Processing emergency query... Please wait.\", \n",
    "            'hybrid': \"üîÑ **HYBRID** - Processing emergency query... Please wait.\"\n",
    "        }\n",
    "        \n",
    "        # Yield initial state immediately\n",
    "        yield results['vector_store'], results['knowledge_graph'], results['hybrid']\n",
    "        \n",
    "        # Create thread-safe variables\n",
    "        results_lock = threading.Lock()\n",
    "        completion_times = {}\n",
    "        completed_count = 0\n",
    "        \n",
    "        def process_single_result(retriever_type, question, call_transcript, load_persist_vector, load_persist_kg):\n",
    "            \"\"\"Process a single retriever and return the result\"\"\"\n",
    "            thread_start = time.time()\n",
    "            try:\n",
    "                result = run_single_retriever(question, call_transcript, retriever_type, load_persist_vector, load_persist_kg)\n",
    "                exec_time = time.time() - thread_start\n",
    "                enhanced_result = f\"‚ö° **Completed in {exec_time:.2f}s** | **{retriever_type.replace('_', ' ').title()}**\\n\\n{result}\"\n",
    "                return retriever_type, enhanced_result, exec_time, True\n",
    "            except Exception as e:\n",
    "                exec_time = time.time() - thread_start\n",
    "                error_result = f\"‚ùå **FAILED in {exec_time:.2f}s** | **{retriever_type.replace('_', ' ').title()}**\\n\\n**Error:** {str(e)}\"\n",
    "                return retriever_type, error_result, exec_time, False\n",
    "        \n",
    "        # Start all three workflows concurrently\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            # Submit all tasks\n",
    "            futures = {\n",
    "                executor.submit(process_single_result, 'vector_store', question, call_transcript, load_persist_vector, load_persist_kg): 'vector_store',\n",
    "                executor.submit(process_single_result, 'knowledge_graph', question, call_transcript, load_persist_vector, load_persist_kg): 'knowledge_graph',\n",
    "                executor.submit(process_single_result, 'hybrid', question, call_transcript, load_persist_vector, load_persist_kg): 'hybrid'\n",
    "            }\n",
    "            \n",
    "            # Process results as they complete and yield immediately\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    retriever_type, result, exec_time, success = future.result()\n",
    "                    \n",
    "                    with results_lock:\n",
    "                        completed_count += 1\n",
    "                        results[retriever_type] = result\n",
    "                        if success:\n",
    "                            completion_times[retriever_type] = exec_time\n",
    "                        \n",
    "                        print(f\"‚úÖ {retriever_type} completed in {exec_time:.2f}s ({completed_count}/3)\")\n",
    "                    \n",
    "                    # Yield updated results immediately - this is the key!\n",
    "                    yield results['vector_store'], results['knowledge_graph'], results['hybrid']\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Future execution error: {str(e)}\")\n",
    "                    # Still yield the current state even if there's an error\n",
    "                    yield results['vector_store'], results['knowledge_graph'], results['hybrid']\n",
    "        \n",
    "        # Final processing - add trophy to fastest result\n",
    "        if completion_times:\n",
    "            fastest_time = min(completion_times.values())\n",
    "            fastest_retriever = min(completion_times.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            with results_lock:\n",
    "                if fastest_retriever in results:\n",
    "                    results[fastest_retriever] = f\"üèÜ **FASTEST** | {results[fastest_retriever]}\"\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"üéâ All emergency retrievers completed! Total time: {total_time:.2f}s\")\n",
    "        \n",
    "        # Final yield with trophy added\n",
    "        yield results['vector_store'], results['knowledge_graph'], results['hybrid']\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    with gr.Blocks(title=\"Emergency Dispatch RAG Interface\", theme=gr.themes.Soft()) as interface:\n",
    "        \n",
    "        gr.Markdown(\"# üö® Emergency Dispatch RAG Workflow Interface\")\n",
    "        gr.Markdown(\"This is an enhanced Retrieval-Augmented Generation (RAG) interface for **medical emergencies and emergency dispatch**. The system can handle both regular medical queries and emergency call scenarios with structured emergency analysis.\")\n",
    "        gr.Markdown(\"**‚ö° Real-time Emergency Processing:** When you click 'Process Emergency Query', all three retriever types (Vector Store, Knowledge Graph, and Hybrid) run **in parallel** and results appear **immediately** as each one finishes! Emergency scenarios get special analysis and structured reports.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                # Input section\n",
    "                gr.Markdown(\"### üìù Input Section\")\n",
    "                \n",
    "                question_input = gr.Textbox(\n",
    "                    label=\"üîç Question / Query\",\n",
    "                    placeholder=\"Enter your medical question or emergency query here...\",\n",
    "                    lines=3,\n",
    "                    value=\"how to revive a person who is unconscious\"\n",
    "                )\n",
    "                \n",
    "                call_transcript_input = gr.Textbox(\n",
    "                    label=\"üéôÔ∏è Emergency Call Transcript (Optional)\",\n",
    "                    placeholder=\"Paste emergency call transcript here for detailed emergency analysis...\",\n",
    "                    lines=5,\n",
    "                    value=\"\"\n",
    "                )\n",
    "                \n",
    "                gr.Markdown(\"### ‚öôÔ∏è Configuration\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    load_persist_vector = gr.Textbox(\n",
    "                        label=\"üì¶ Vector Store Path\",\n",
    "                        value=\"./vectorstore\",\n",
    "                        placeholder=\"Path to vector store\"\n",
    "                    )\n",
    "                    \n",
    "                    load_persist_kg = gr.Textbox(\n",
    "                        label=\"üï∏Ô∏è Knowledge Graph Store Path\", \n",
    "                        value=\"./kgstore_test_50\",\n",
    "                        placeholder=\"Path to knowledge graph store\"\n",
    "                    )\n",
    "                \n",
    "                submit_btn = gr.Button(\"üö® Process Emergency Query\", variant=\"primary\", size=\"lg\")\n",
    "                clear_btn = gr.Button(\"üßπ Clear All\", variant=\"secondary\")\n",
    "            \n",
    "            with gr.Column(scale=3):\n",
    "                # Output section\n",
    "                gr.Markdown(\"### üìä Emergency Analysis Results\")\n",
    "                \n",
    "                with gr.Tabs():\n",
    "                    with gr.TabItem(\"üè™ Vector Store\"):\n",
    "                        vector_output = gr.Textbox(\n",
    "                            label=\"Vector Store Emergency Results\",\n",
    "                            lines=25,\n",
    "                            max_lines=30,\n",
    "                            show_copy_button=True\n",
    "                        )\n",
    "                    \n",
    "                    with gr.TabItem(\"üï∏Ô∏è Knowledge Graph\"):\n",
    "                        kg_output = gr.Textbox(\n",
    "                            label=\"Knowledge Graph Emergency Results\",\n",
    "                            lines=25,\n",
    "                            max_lines=30,\n",
    "                            show_copy_button=True\n",
    "                        )\n",
    "                    \n",
    "                    with gr.TabItem(\"üîÑ Hybrid\"):\n",
    "                        hybrid_output = gr.Textbox(\n",
    "                            label=\"Hybrid Emergency Results\",\n",
    "                            lines=25,\n",
    "                            max_lines=30,\n",
    "                            show_copy_button=True\n",
    "                        )\n",
    "        \n",
    "        # Event handlers with streaming\n",
    "        submit_btn.click(\n",
    "            fn=process_emergency_question_streaming,\n",
    "            inputs=[question_input, call_transcript_input, load_persist_vector, load_persist_kg],\n",
    "            outputs=[vector_output, kg_output, hybrid_output]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=lambda: (\"\", \"\", \"\", \"\", \"\", \"\", \"\"),\n",
    "            outputs=[question_input, call_transcript_input, vector_output, kg_output, hybrid_output, load_persist_vector, load_persist_kg]\n",
    "        )\n",
    "        \n",
    "        # Example sections\n",
    "        gr.Markdown(\"### üí° Example Emergency Queries\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"**ü©∫ Medical Emergency Examples:**\")\n",
    "                medical_examples = [\n",
    "                    \"how to revive a person who is unconscious\",\n",
    "                    \"what are the steps for CPR?\",\n",
    "                    \"how to treat a burn injury?\",\n",
    "                    \"what to do in case of choking?\",\n",
    "                    \"what are the symptoms of a heart attack?\",\n",
    "                ]\n",
    "                \n",
    "                for example in medical_examples:\n",
    "                    gr.Button(example, size=\"sm\").click(\n",
    "                        fn=lambda x=example: x,\n",
    "                        outputs=question_input\n",
    "                    )\n",
    "            \n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"**üö® Emergency Dispatch Examples:**\")\n",
    "                dispatch_examples = [\n",
    "                    \"person collapsed in public place\",\n",
    "                    \"car accident with injuries reported\", \n",
    "                    \"house fire with people trapped\",\n",
    "                    \"suspected heart attack patient\",\n",
    "                    \"motorcycle accident victim unconscious\",\n",
    "                ]\n",
    "                \n",
    "                for example in dispatch_examples:\n",
    "                    gr.Button(example, size=\"sm\").click(\n",
    "                        fn=lambda x=example: x,\n",
    "                        outputs=question_input\n",
    "                    )\n",
    "        \n",
    "        gr.Markdown(\"### üìû Sample Emergency Call Transcript\")\n",
    "        sample_transcript = \"\"\"Dispatcher: 911, what's your emergency?\n",
    "        Caller: Hi, I just found someone unconscious in the park. I need help!\n",
    "        Dispatcher: Okay, stay calm. Can you tell me your location?\n",
    "        aller: Yes, I'm at Riverside Park, near the north entrance by the playground.\n",
    "        Dispatcher: Got it. Are they breathing?\n",
    "        Caller: Um, I'm not sure. Let me check... No, I don't think so!\"\"\"\n",
    "        \n",
    "        gr.Button(\"üìã Use Sample Emergency Call\", size=\"sm\").click(\n",
    "            fn=lambda: sample_transcript,\n",
    "            outputs=call_transcript_input\n",
    "        )\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        ### üîß How It Works:\n",
    "        1. **Emergency Detection**: The system automatically detects if your input is emergency-related\n",
    "        2. **Call Analysis**: If a call transcript is provided, detailed emergency analysis is performed\n",
    "        3. **Structured Reports**: Emergency scenarios generate structured dispatch reports\n",
    "        4. **Concurrent RAG**: All retriever types run in parallel for fastest response\n",
    "        5. **Real-time Updates**: See results as soon as each retriever completes\n",
    "        \"\"\")\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Usage example:\n",
    "interface = build_emergency_rag_interface(build_emergency_rag_workflowv2, model, nodes)\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    interface.launch(\n",
    "        server_name=\"0.0.0.0\",  # Allow external access\n",
    "        server_port=7864,       # Different port from original\n",
    "        share=False,            # Set to True for public sharing\n",
    "        debug=True              # Enable debug mode\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
